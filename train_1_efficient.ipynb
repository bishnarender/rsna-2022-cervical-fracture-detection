{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding JPEG images and decoding/encoding RLE datasets\n",
    "# !pip3 install -q pylibjpeg==1.4.0\n",
    "# https://github.com/pydicom/pylibjpeg\n",
    "# !pip3 install pydicom==2.3.0\n",
    "## !pip3 install -q python-gdcm==3.0.20\n",
    "# !pip3 install -q torchvision==0.12.0\n",
    "# !pip3 install -q torchaudio==0.11.0\n",
    "# !pip3 install -q torchmetrics==0.11.0\n",
    "# !pip3 install -q torchtext==0.12.0\n",
    "# !pip3 install -q torch==1.13.1 # cuda 'sm_86' is only supported by 1.13.1\n",
    "# !pip3 install -q timm==0.6.12 # or 0.4.12 (for train_1.ipynb and train_1_efficient.ipynb)\n",
    "# !pip3 -q install monai==1.1.0\n",
    "# !pip3 -q install segmentation-models-pytorch==0.2.1\n",
    "# !conda install -c pytorch magma-cuda110==2.5.2\n",
    "# !pip3 install opencv-python==4.5.4.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"]=\"1\"\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suitable for kaggle notebook\n",
    "# sys.path = ['../ca_2',] + sys.path\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, ast, cv2, time, pickle, random\n",
    "import pylibjpeg\n",
    "# import gdcm\n",
    "import pydicom\n",
    "# pydicom is a pure Python package for working with DICOM files. \n",
    "# -It lets you read, modify and write DICOM data in an easy \"pythonic\" way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import nibabel as nib\n",
    "# read / write access to some common neuroimaging file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na/miniconda3/envs/base_2/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/na/miniconda3/envs/base_2/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from monai.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  monai.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip3 install torchview\n",
    "# from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', None) # 500\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# benchmark mode is good whenever your input sizes for your network do not vary. \n",
    "# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_type = 'timm3d_effv2_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "n_folds = 5\n",
    "\n",
    "# check models performance => https://github.com/rwightman/pytorch-image-models/blob/main/results/results-imagenet.csv\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k' # 'resnet18d' # 'tf_efficientnetv2_s_in21ft1k <=> tf_efficientnetv2_s.in21k_ft_in1k'\n",
    "\n",
    "image_sizes = [128, 128, 128]\n",
    "R = Resize(image_sizes, mode=\"area\") # monai => Resize\n",
    "\n",
    "init_lr = 19e-5 # 19e-4\n",
    "eta_min = 11e-5\n",
    "batch_size = 3\n",
    "drop_rate = 0. # avoid this when running over kaggle.\n",
    "drop_path_rate = 0.\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1 # to avoid overfitting\n",
    "\n",
    "data_dir = './' # ../input/rsna-2022-cervical-spine-fracture-detection\n",
    "use_amp = True\n",
    "num_workers = 12\n",
    "out_dim = 7\n",
    "\n",
    "n_epochs = 1000 # 1000\n",
    "\n",
    "log_dir = './logs'\n",
    "model_dir = './models'\n",
    "model_dir_seg = './kaggle'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
    "    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
    "    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
    "])\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "])\n",
    "\n",
    "# transforms_train =''\n",
    "# transforms_valid =''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    #            StudyInstanceUID    patient_overall    C1  C2  C3  C4  C5  C6  C7\n",
    "    # 0   1.2.826.0.1.3680043.6200                 1    1   1   0   0   0   0   0\n",
    "\n",
    "mask_files = os.listdir(f'{data_dir}segmentations')\n",
    "\n",
    "df_mask = pd.DataFrame({ 'mask_file': mask_files, })\n",
    "    #     mask_file\n",
    "    # 0   1.2.826.0.1.3680043.30487.nii\n",
    "    # 1   1.2.826.0.1.3680043.30640.nii\n",
    "\n",
    "df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x[:-4])\n",
    "    #     mask_file                       StudyInstanceUID\n",
    "    # 0   1.2.826.0.1.3680043.30487.nii   1.2.826.0.1.3680043.30487\n",
    "\n",
    "df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir, 'segmentations', x))\n",
    "    #     mask_file                                       StudyInstanceUID\n",
    "    # 0   ./segmentations/1.2.826.0.1.3680043.30487.nii   1.2.826.0.1.3680043.30487\n",
    "\n",
    "\n",
    "df_train = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n",
    "    #             StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  mask_file\n",
    "    # 0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0   Nan\n",
    "    # 1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0   Nan\n",
    "del df_mask\n",
    "\n",
    "df_train['image_folder'] = df_train['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "    #             StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  mask_file  image_folder\n",
    "    # 0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0   Nan       ./train_images/1.2.826.0.1.3680043.6200      \n",
    "    # 1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0   Nan       ./train_images/1.2.826.0.1.3680043.27262\n",
    "\n",
    "df_train['mask_file'].fillna('', inplace=True)\n",
    "    #             StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  mask_file  image_folder\n",
    "    # 0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0             ./train_images/1.2.826.0.1.3680043.6200      \n",
    "    # 1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0             ./train_images/1.2.826.0.1.3680043.27262\n",
    "    \n",
    "df_seg = df_train.query('mask_file != \"\"').reset_index(drop=True)\n",
    "    # dropping rows with df['mask_file'] == ''\n",
    "    # equivalent to \n",
    "    # df_seg = df_train[df_train['mask_file'] != \"\"].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "kf = KFold(5) # n_splits=5\n",
    "df_seg['fold'] = -1\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)): # X=df_seg, y =df_seg\n",
    "    df_seg.loc[valid_idx, 'fold'] = fold # df_seg.loc[row_index, column_fold] = fold_number\n",
    "# fold column represents the split_number / fold_number in which row falls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>image_folder</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [StudyInstanceUID, patient_overall, C1, C2, C3, C4, C5, C6, C7, mask_file, image_folder, fold]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2.826.0.1.3680043.20574 not present in the dataset\n",
    "df_seg[df_seg['StudyInstanceUID'] == '1.2.826.0.1.3680043.20574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>image_folder</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.1363</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.1363.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.1363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.25704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.25704.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.25704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.20647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.20647.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.20647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.31077</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.31077.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.31077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n",
       "0   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0   \n",
       "1  1.2.826.0.1.3680043.25704                0   0   0   0   0   0   0   0   \n",
       "2  1.2.826.0.1.3680043.20647                0   0   0   0   0   0   0   0   \n",
       "3  1.2.826.0.1.3680043.31077                1   0   0   1   1   1   1   0   \n",
       "\n",
       "                                       mask_file  \\\n",
       "0   ./segmentations/1.2.826.0.1.3680043.1363.nii   \n",
       "1  ./segmentations/1.2.826.0.1.3680043.25704.nii   \n",
       "2  ./segmentations/1.2.826.0.1.3680043.20647.nii   \n",
       "3  ./segmentations/1.2.826.0.1.3680043.31077.nii   \n",
       "\n",
       "                               image_folder  fold  \n",
       "0   ./train_images/1.2.826.0.1.3680043.1363     0  \n",
       "1  ./train_images/1.2.826.0.1.3680043.25704     0  \n",
       "2  ./train_images/1.2.826.0.1.3680043.20647     0  \n",
       "3  ./train_images/1.2.826.0.1.3680043.31077     0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>image_folder</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.2.826.0.1.3680043.28025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.28025.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.28025</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.2.826.0.1.3680043.21321</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.21321.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.21321</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.2.826.0.1.3680043.26990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>./segmentations/1.2.826.0.1.3680043.26990.nii</td>\n",
       "      <td>./train_images/1.2.826.0.1.3680043.26990</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n",
       "84  1.2.826.0.1.3680043.28025                0   0   0   0   0   0   0   0   \n",
       "85  1.2.826.0.1.3680043.21321                1   1   1   1   0   0   0   1   \n",
       "86  1.2.826.0.1.3680043.26990                1   0   0   0   0   1   1   1   \n",
       "\n",
       "                                        mask_file  \\\n",
       "84  ./segmentations/1.2.826.0.1.3680043.28025.nii   \n",
       "85  ./segmentations/1.2.826.0.1.3680043.21321.nii   \n",
       "86  ./segmentations/1.2.826.0.1.3680043.26990.nii   \n",
       "\n",
       "                                image_folder  fold  \n",
       "84  ./train_images/1.2.826.0.1.3680043.28025     4  \n",
       "85  ./train_images/1.2.826.0.1.3680043.21321     4  \n",
       "86  ./train_images/1.2.826.0.1.3680043.26990     4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_seg.to_csv('df_seg_train_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_seg = pd.read_csv('df_seg_train_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_seg.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the list of inverted (z-axis) segmentation masks:\n",
    "revert_list = [\n",
    "    '1.2.826.0.1.3680043.1363',\n",
    "    '1.2.826.0.1.3680043.20120',\n",
    "    '1.2.826.0.1.3680043.2243',\n",
    "    '1.2.826.0.1.3680043.24606',\n",
    "    '1.2.826.0.1.3680043.32071'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    # path => ./train_images/1.2.826.0.1.3680043.1363/1.dcm       \n",
    "    dicom = pydicom.read_file(path)\n",
    "    #     dicom =>\n",
    "    #     Dataset.file_meta -------------------------------\n",
    "    #     (0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
    "    #     (0002, 0002) Media Storage SOP Class UID         UI: CT Image Storage\n",
    "    #     (0002, 0003) Media Storage SOP Instance UID      UI: 1.2.826.0.1.3680043.1363.1.1\n",
    "    #     (0002, 0010) Transfer Syntax UID                 UI: Explicit VR Little Endian\n",
    "    #     (0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1.1\n",
    "    #     (0002, 0013) Implementation Version Name         SH: 'PYDICOM 2.3.0'\n",
    "    #     -------------------------------------------------\n",
    "    #     (0008, 0018) SOP Instance UID                    UI: 1.2.826.0.1.3680043.1363.1.1\n",
    "    #     (0008, 0023) Content Date                        DA: '20220727'\n",
    "    #     (0008, 0033) Content Time                        TM: '183924.353110'\n",
    "    #     (0010, 0010) Patient's Name                      PN: '1363'\n",
    "    #     (0010, 0020) Patient ID                          LO: '1363'\n",
    "    #     (0018, 0050) Slice Thickness                     DS: '1.0'\n",
    "    #     (0020, 000d) Study Instance UID                  UI: 1.2.826.0.1.3680043.1363\n",
    "    #     (0020, 000e) Series Instance UID                 UI: 1.2.826.0.1.3680043.1363.1\n",
    "    #     (0020, 0013) Instance Number                     IS: '1'\n",
    "    #     (0020, 0032) Image Position (Patient)            DS: [-149.2080078125, -350.2080078125, 54]\n",
    "    #     (0020, 0037) Image Orientation (Patient)         DS: [1, 0, 0, 0, 1, 0]\n",
    "    #     (0028, 0002) Samples per Pixel                   US: 1\n",
    "    #     (0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
    "    #     (0028, 0010) Rows                                US: 512\n",
    "    #     (0028, 0011) Columns                             US: 512\n",
    "    #     (0028, 0030) Pixel Spacing                       DS: [0.583984375, 0.583984375]\n",
    "    #     (0028, 0100) Bits Allocated                      US: 16\n",
    "    #     (0028, 0101) Bits Stored                         US: 12\n",
    "    #     (0028, 0102) High Bit                            US: 11\n",
    "    #     (0028, 0103) Pixel Representation                US: 0\n",
    "    #     (0028, 1050) Window Center                       DS: [450, 40]\n",
    "    #     (0028, 1051) Window Width                        DS: [1500, 350]\n",
    "    #     (0028, 1052) Rescale Intercept                   DS: '-1024.0'\n",
    "    #     (0028, 1053) Rescale Slope                       DS: '1.0'\n",
    "    #     (7fe0, 0010) Pixel Data                          OW: Array of 524288 elements    \n",
    "       \n",
    "    data = dicom.pixel_array\n",
    "        # data => \n",
    "        # [[ 22  35  53 ...  46  80  33]\n",
    "        #  [ 49  12  44 ...  58  52  11]    \n",
    "        #          ...\n",
    "        #          ...\n",
    "        #  [11  54  31 ... 112  64  99]\n",
    "        #  [371 280 242 ...  67  24   0]] \n",
    "        # data.shape => (512, 512) i.e., (Rows, Columns)    \n",
    "        # np.unique(data) => [0    1    2    3    4    5    ......   3156 3163 3182 3200 3228 3267 3274]\n",
    "\n",
    "    \n",
    "    data = cv2.resize(data, (image_sizes[0], image_sizes[1]), interpolation = cv2.INTER_AREA)\n",
    "        # If you are enlarging the image, you should prefer to use INTER_LINEAR or INTER_CUBIC interpolation. \n",
    "        # If you are shrinking the image, you should prefer to use INTER_AREA interpolation.    \n",
    "        # data =>\n",
    "        # [[ 40  45  57 ...  30  27  44]\n",
    "        #  [ 40  50  52 ...  38  26  26]\n",
    "        #      ...\n",
    "        #      ...\n",
    "        #  [ 99 158 234 ... 151 159 180]\n",
    "        #  [155  91  74 ...  50 134 103]] \n",
    "        # data.shape => (128,128) i.e., (image_sizes[0], image_sizes[1])\n",
    "        # np.unique(data) => [18   21   22   23   24   25   26   27   ........ 2684 2716 2727 2755 2791 2875]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "    # path => ./train_images/1.2.826.0.1.3680043.1363\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(path, \"*\")),\\\n",
    "                     key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    # glob(os.path.join(path, \"*\")) => ['./train_images/1.2.826.0.1.3680043.1363/179.dcm', \n",
    "    #                                        './train_images/1.2.826.0.1.3680043.1363/174.dcm',..........    \n",
    "    # \"./train_images/1.2.826.0.1.3680043.1363/179.dcm\".split('/')[-1].split(\".\")[0] => 179\n",
    "    # t_paths => ['./train_images/1.2.826.0.1.3680043.1363/1.dcm', \n",
    "    #                                        './train_images/1.2.826.0.1.3680043.1363/2.dcm',..........\n",
    "    \n",
    "\n",
    "    n_scans = len(t_paths) # number of scans in path; e.g. 199\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_sizes[2])).round().astype(int)\n",
    "    # np.linspace(0., 1., 128) => array([0.        , 0.00787402, 0.01574803, 0.02362205, ...., 1. ])\n",
    "    # np.linspace(start, stop, num_of_samples_to_generate, ...) => Return evenly spaced numbers over a specified interval.\n",
    "    # image_sizes[2] => defined in 'config' section above\n",
    "    # list(range(199)) = [0,1,2,3,.......,199]\n",
    "    # np.quantile([0,1,2,3,.......,199], array([0.        , 0.00787402, 0.01574803, 0.02362205, ...., 1. ]))\n",
    "    # np.quantile(Input,  sequence of quantiles to compute between 0 and 1, ...)\n",
    "    # indices => [  0   2   3   5   6   8   9  11  12  14  16  17  19  20  22  23  25  27 ............ 198]    \n",
    "    # len(indices) => 128\n",
    "    # even if n_scans is less than 128, the len(indices) remained 128. As, quantiles are repeated.\n",
    "    \n",
    "    t_paths = [t_paths[i] for i in indices] \n",
    "    # selecting 'image_sizes[2]' no. of elements from t_paths\n",
    "    # previous 199 now 128\n",
    "    \n",
    "\n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    # images => list all images, where each image is in 'array' form with shape (128,128).\n",
    "\n",
    "    images = np.stack(images, 0)\n",
    "    # np.stack(arrays, axis, out=None, ...) => Join a sequence of arrays along a new axis.    \n",
    "    #  images.shape => (128, 128, 128) i.e., (z,x,y)\n",
    "    #  first axis is z axis (represents number of images) and (x,y) is per image shape.\n",
    "    # images => \n",
    "    # [[[ 39  30  37 ...  30  40  37]\n",
    "    #   [ 32  38  29 ...  27  36  34]\n",
    "    #   [ 27  36  40 ...  30  30  33]\n",
    "\n",
    "    #   ...\n",
    "    #   ...\n",
    "    #   ...\n",
    "\n",
    "    #   [ 66  71  83 ...  78  69  68]\n",
    "    #   [ 74  67  63 ...  63  60  68]\n",
    "    #   [ 69  73  65 ...  72  74  71]]]  \n",
    "    \n",
    "    \n",
    "    # normalization section\n",
    "    images = images - np.min(images)\n",
    "        # np.min(images) => minimum value in (z,x,y) matrix    \n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "        # images=>\n",
    "        # [[[0.00953098 0.00727364 0.00902935 ... 0.00727364 0.00978179 0.00902935]\n",
    "        #   [0.00777527 0.00928016 0.00702282 ... 0.00652119 0.00877853 0.0082769 ]\n",
    "        #   [0.00652119 0.00877853 0.00978179 ... 0.00727364 0.00727364 0.00802608]\n",
    "        #   ...\n",
    "        #   ...\n",
    "\n",
    "    # normalized values are too low thus multiplying by 255 to stay within the range.\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "        # prior to any type of transformation(resize, augmentation etc.) convert data to uint8.    \n",
    "        \n",
    "        # with \".astype(np.uint8)\", images =>\n",
    "        # [[[ 2  1  2 ...  1  2  2]\n",
    "        #   [ 1  2  1 ...  1  2  2]\n",
    "        #   [ 1  2  2 ...  1  1  2]    \n",
    "        #   ...\n",
    "        #   ...\n",
    "\n",
    "        # without \".astype(np.uint8)\", images =>\n",
    "        # [[[ 2.43039874  1.85477798  2.30248301 ...  1.85477798  2.4943566  2.30248301]\n",
    "        #   [ 1.9826937   2.36644087  1.79082012 ...  1.6629044   2.23852515  2.11060943]\n",
    "        #   [ 1.6629044   2.23852515  2.4943566  ...  1.85477798  1.85477798  2.04665157]\n",
    "        #     ...\n",
    "        #     ...\n",
    "\n",
    "    return images, indices\n",
    "\n",
    "\n",
    "def load_sample(row, has_mask=True):\n",
    "    # row =>\n",
    "    # index                                                          0\n",
    "    # StudyInstanceUID                        1.2.826.0.1.3680043.1363\n",
    "    # patient_overall                                                1\n",
    "    # C1                                                             0\n",
    "    # C2                                                             0\n",
    "    # C3                                                             0\n",
    "    # C4                                                             0\n",
    "    # C5                                                             1\n",
    "    # C6                                                             0\n",
    "    # C7                                                             0\n",
    "    # mask_file           ./segmentations/1.2.826.0.1.3680043.1363.nii\n",
    "    # image_folder             ./train_images/1.2.826.0.1.3680043.1363\n",
    "    # fold                                                           0\n",
    "    # Name: 0, dtype: object    \n",
    "    \n",
    "    image, index = load_dicom_line_par(row.image_folder)\n",
    "    # image.shape => (128,128,128)\n",
    "    # image.ndim => 3    \n",
    "    \n",
    "    if image.ndim < 4: # number of dimension is less 4\n",
    "        image = np.expand_dims(image, axis=0).repeat(repeats=3, axis=0)  # to 3ch\n",
    "        # np.expand_dims(image, axis=0) => (1,128,128,128)\n",
    "        # image.shape => (3,128,128,128) i.e., repeatition of (128,128,128) three times\n",
    "        # image.ndim => 4            \n",
    "\n",
    "    if has_mask:\n",
    "        # reading nii file\n",
    "        # to ask for the array data is to call the get_fdata()\n",
    "        mask_org = nib.load(row.mask_file).get_fdata()\n",
    "            # np.min(mask_org, np.max(mask_org) => 0.0, 10.0\n",
    "            # np.unique(mask_org) => [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
    "        \n",
    "        shape = mask_org.shape\n",
    "            # type(mask_org) => <class 'numpy.memmap'> \n",
    "            # shape => (512, 512, 199) i.e., (0,1,2)\n",
    "            # 199 number of slices/images (each patient have different-2 slices/images)\n",
    "                        \n",
    "            \n",
    "        if row.StudyInstanceUID in revert_list:\n",
    "            # trasposing such that mask will be in orientation with image\n",
    "            mask_org = mask_org.transpose((2, 1, 0))[:, ::-1, :]  \n",
    "                # np.transpose(a, axes=(2, 1, 0)) => Returns an array with axes transposed.\n",
    "                # mask_org.shape => (199, 512, 512)                        \n",
    "        else:\n",
    "            # trasposing such that mask will be in orientation with image\n",
    "            mask_org = mask_org.transpose((2, 1, 0))[::-1, ::-1, :-1:]  \n",
    "                # np.transpose(a, axes=(2, 1, 0)) => Returns an array with axes transposed.\n",
    "                # mask_org.shape => (199, 512, 512)            \n",
    "            \n",
    "        mask_org = mask_org[index]  # picking same indexes from 199, as in images\n",
    "            # mask_org.shape => (128, 512, 512)        \n",
    "        \n",
    "        shape = mask_org.shape\n",
    "\n",
    "        mask = np.zeros((7, shape[0], shape[1], shape[2]))\n",
    "            # mask.shape => (7,128,512,512) # we only intereseted in seven mask for (seven cervical vertebrae C1 to C7)   \n",
    "        for cId in range(7):\n",
    "            mask[cId] = (mask_org == (cId+1))\n",
    "                # (mask_org == (cid+1)) ==>\n",
    "                # [[[False False False ... False False False]\n",
    "                #   [False False False ... False False False]\n",
    "                #   [False False False ... False False False]\n",
    "                #   ...\n",
    "                #   ...\n",
    "                #   [False False False ... False False False]\n",
    "                #   [False False False ... False False False]\n",
    "                #   [False False False ... False False False]]]\n",
    "                \n",
    "        # mask.shape => (7, 128, 512, 512)\n",
    "        # np.min(mask), np.max(mask) => 0., 1.\n",
    "\n",
    "        mask = mask.astype(np.uint8) * 255        \n",
    "        # np.min(mask), np.max(mask) => 0, 255        \n",
    "        # np.unique(mask) => [0 255]        \n",
    "            \n",
    "        shape = mask.shape    \n",
    "            \n",
    "            # mask.shape => (7, 128, 512, 512) => 7 = channels, [128,512,512] = spatial dimension \n",
    "        mask = R(mask).numpy() \n",
    "            # type(mask), mask.shape, np.min(mask), np.max(mask)  => np.ndarray, (7, 128, 128, 128), 0, 255\n",
    "            # np.unique(mask) => [0  15  31  47  63  79  95 111 127 143 159 175 191 207 223 239 255]         \n",
    "\n",
    "        return image, mask\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "class SEGDataset(torch.utils.data.Dataset): \n",
    "# An abstract class representing a Dataset.\n",
    "# An abstract class is a class, but not one you can create objects from directly. Its purpose is to define how other classes should look like, i.e. what methods and properties they are expected to have.\n",
    "    def __init__(self, df, mode, transform):\n",
    "        \n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]                \n",
    "\n",
    "#         image, mask = load_sample(row, has_mask=True)\n",
    "            \n",
    "#             # np.min(image), np.max(image) => 0 254\n",
    "#             # mask.shape, np.min(mask), np.max(mask) => (7, 128, 128, 128), 0, 255\n",
    "#             # np.unique(mask) => [ 0  15  31  47  63  79  95 111 127 143 159 175 191 207 223 239 255]\n",
    "#         res = self.transform({'image':image, 'mask':mask}) # returns torch array\n",
    "#         image = res['image'] / 255. # scaling high image values otherwise no display. \n",
    "#         mask = res['mask']\n",
    "# #         image = image / 255.  # for view without transform\n",
    "        \n",
    "#             # mask.shape, np.min(mask), np.max(mask) => torch.Size([7, 128, 128, 128]), 0.0, 255.0 \n",
    "#             # np.unique(mask) => [0.  15.  31.  47.  63.  79.  95. 111. 127. 143. 159. 175. 191. 207. 223. 239. 255.]\n",
    "#         mask = (mask > 127).astype(np.float32) # 255/2 = 127.5 so convert '<127 to 0' and '>127 to 1'.\n",
    "#             # np.min(image), np.max(image) => 0.0, 1.0\n",
    "#             # mask.shape, np.min(mask), np.max(mask), np.unique(mask) => (7, 128, 128, 128), 0.0, 1.0, [0. 1.]\n",
    "        \n",
    "        ### using local cache\n",
    "        image_file = os.path.join(data_dir, f'numpy/{row.StudyInstanceUID}.npy')\n",
    "        mask_file = os.path.join(data_dir, f'numpy/{row.StudyInstanceUID}_mask.npy')\n",
    "#         np.save(image_file, image)\n",
    "#         np.save(mask_file, mask)\n",
    "        \n",
    "        image = np.load(image_file)\n",
    "        mask = np.load(mask_file)\n",
    "                \n",
    "        image, mask = torch.tensor(image).float(), torch.tensor(mask).float()        \n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # storing image, mask => in numpy formats\n",
    "# df_show = df_seg\n",
    "# dataset_show = SEGDataset(df_show, 'train', transform=transforms_train)\n",
    "# for index, row in df_show.iterrows():\n",
    "#     img, mask = dataset_show[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_show = df_seg[0:4] # testing only few value\n",
    "# dataset_show = SEGDataset(df_show, 'train', transform=transforms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging\n",
    "# img, mask = dataset_show[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # viewing some images\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "# for i in range(1): # 2\n",
    "#     f, axarr = plt.subplots(1,8)\n",
    "#     for p in range(4):\n",
    "#         idx = i*4+p\n",
    "#         img, mask = dataset_show[idx]\n",
    "#             # img.shape, mask.shape => (3, 128, 128, 128) (7, 128, 128, 128)        \n",
    "        \n",
    "#         img = img[:, 60, :, :] # checking 60th dcm image for a particular patient       \n",
    "#         mask = mask[:, 60, :, :] # checking corresponding 60th slice \n",
    "#             # img.shape, mask.shape => (3, 128, 128) (7, 128, 128)        \n",
    "        \n",
    "#         # merging 7 channels in order to reduce to 3\n",
    "#         mask[0] = mask[0] + mask[3] + mask[6] # merging C1, C4 and C7\n",
    "#         mask[1] = mask[1] + mask[4] # merging C2, C5\n",
    "#         mask[2] = mask[2] + mask[5] # merging C3, C6\n",
    "        \n",
    "#         mask = mask[:3] # selecting only 3 sequence / channels out of 7.\n",
    "#             # mask.shape => (3, 128, 128)\n",
    "\n",
    "\n",
    "#         axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze()) # squeeze(); removes axes of length one.\n",
    "#         axarr[p+4].imshow(mask.transpose(0, 1).transpose(1,2).squeeze())        \n",
    "\n",
    "            \n",
    "# for i in range(1): # 2\n",
    "#     f, axarr = plt.subplots(1,4)\n",
    "#     for p in range(4):\n",
    "#         idx = i*4+p\n",
    "#         img, mask = dataset_show[idx]        \n",
    "#         img = img[:, 60, :, :] # checking 60th dcm image for a particular patient       \n",
    "#         mask = mask[:, 60, :, :] # checking corresponding 60th slice \n",
    "        \n",
    "#         # merging 7 channels in order to reduce to 3\n",
    "#         mask[0] = mask[0] + mask[3] + mask[6] # merging C1, C4 and C7\n",
    "#         mask[1] = mask[1] + mask[4] # merging C2, C5\n",
    "#         mask[2] = mask[2] + mask[5] # merging C3, C6\n",
    "        \n",
    "#         mask = mask[:3] # selecting only 3 sequence / channels out of 7.\n",
    "            \n",
    "#         img = img * 0.7 + mask * 0.3 # merging image and its mask\n",
    "#             # img.shape => (3, 128, 128)           \n",
    "#         axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())  # squeeze(); removes axes of length one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):  # nn.Module: Base class for all neural network modules.\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm4smp.create_model(\n",
    "            backbone,\n",
    "            in_chans=3, # number of channels for input\n",
    "            features_only=True, \n",
    "            # output feature maps for selected levels. leave output of encoder.\n",
    "            # default, 5 strides will be output from most of models.\n",
    "            \n",
    "            drop_rate=drop_rate, # set the dropout rate for training.\n",
    "            drop_path_rate=drop_path_rate, # stochastic depth rate. to “deactivate” some layers during training.\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        # self.encoder => for summary\n",
    "        # self.encoder.default_cfg =>        \n",
    "        # {'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21ft1k-d7dafa41.pth', \n",
    "        #  'input_size': (3, 300, 300), 'pool_size': (10, 10), 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), \n",
    "        #  'first_conv': 'conv_stem', 'test_input_size': (3, 384, 384), 'architecture': 'tf_efficientnetv2_s_in21ft1k'}\n",
    "        \n",
    "        # self.encoder.feature_info.module_name() => ['act1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
    "        # self.encoder.feature_info.channels() => [64, 64, 128, 256, 512]\n",
    "        # self.encoder.feature_info.reduction() => [2, 4, 8, 16, 32]        \n",
    "\n",
    "\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        # torch.rand(*size) => Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)[0,1)\n",
    "        # type(g) => list\n",
    "        # len(g) => 5\n",
    "        # g[0].shape, g[1].shape, g[2].shape, g[3].shape, g[4].shape =>\n",
    "        # torch.Size([1, 24, 32, 32]), torch.Size([1, 48, 16, 16]), \n",
    "        # torch.Size([1, 64, 8, 8]), torch.Size([1, 160, 4, 4]),\n",
    "        # torch.Size([1, 256, 2, 2])\n",
    "        # number of channels is consistent with: self.encoder.feature_info.channels()  \n",
    "        \n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        # encoder_channels => [1, 24, 48, 64, 160, 256]        \n",
    "            \n",
    "        decoder_channels = [160, 64, 48, 24, 16]        \n",
    "        \n",
    "        if segtype == 'unet':\n",
    "            # resnet is in list of supported encoders to the smp\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],                \n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                # List of integers which specify in_channels parameter for convolutions used in decoder.                \n",
    "                \n",
    "                n_blocks=n_blocks, # n_blocks=encoder_depth,\n",
    "                attention_type = 'scse',                \n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        # nn.Conv2d(in_channels, out_channels,...) => \n",
    "        # Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x => torch.Size([1, 3, 128, 128, 128]); 1 => batch_size\n",
    "        \n",
    "        enc_features = self.encoder(x)[:n_blocks]\n",
    "        f, d = enc_features[0].shape[0], enc_features[0].device\n",
    "        a = [24,48,64,160]\n",
    "        b = [63,31,15,7]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],1,b[i],b[i]), device=d).float()), dim=2) for i, feat in enumerate(enc_features)]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,1,b[i]), device=d).float()), dim=3) for i, feat in enumerate(enc_features)]      \n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,b[i]+1,1), device=d).float()), dim=4) for i, feat in enumerate(enc_features)]   \n",
    "        \n",
    "        global_features = [0] + enc_features\n",
    "        # global_features[0], global_features[1].shape, global_features[2].shape, global_features[3].shape, global_features[4].shape =>\n",
    "        # 0 torch.Size([1, 24, 63, 63, 63]) torch.Size([1, 48, 31, 31, 31]) torch.Size([1, 64, 15, 15, 15]) torch.Size([1, 160, 7, 7, 7]) \n",
    "\n",
    "        seg_features = self.decoder(*global_features) # (*global_features) is equivalent to (global_features[0], global_features[1], global_features[2]...)\n",
    "        # seg_features.shape => torch.Size([1, 32, 128, 128, 128])\n",
    "\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        # seg_features.shape => torch.Size([1, 7, 128, 128, 128])\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        \n",
    "        # module => BatchNorm2d\n",
    "        # affine – a boolean value that when set to True, this module has learnable affine parameters.\n",
    "        # parameters weight and bias are only defined if the argument affine is set to True.\n",
    "        if module.affine:\n",
    "            # torch.no_grad() temporarily sets all of the requires_grad flags to false.\n",
    "            # 'requires_grad' flag is set then model will compute gradient w.r.t to parameter.\n",
    "            with torch.no_grad():\n",
    "            # with => ensures that resource is \"cleaned up\" when the code that uses it finishes running, even if exceptions are thrown.\n",
    "            # with torch.no_grad() => disable gradient calculation in this context.\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module( name, convert_3d(child) )\n",
    "    del module\n",
    "\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 128, 128, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = TimmSegModel(backbone)\n",
    "#     # m => <class '__main__.TimmSegModel'>\n",
    "#     # TimmSegModel(\n",
    "#     #   (encoder): FeatureListNet(\n",
    "#     #     (conv1): Sequential(\n",
    "#     #       (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "#     #  .....\n",
    "#     #  .....\n",
    "\n",
    "# # all 2d layers are converted to 3d\n",
    "m = convert_3d(m)\n",
    "#     # m => <class '__main__.TimmSegModel'>\n",
    "#     # TimmSegModel(\n",
    "#     #   (encoder): FeatureListNet(\n",
    "#     #     (conv1): Sequential(\n",
    "#     #       (0): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
    "#     #  .....\n",
    "#     #  .....\n",
    "\n",
    "m(torch.rand(1, 3, 128,128,128)).shape\n",
    "# # m(torch.rand(1, 3, 128,128,128)).shape => torch.Size([1, 7, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_graph(m, input_data = torch.rand(1, 3, 128, 128, 128), expand_nested=True, save_graph=True).visual_graph\n",
    "# draw_graph(m, input_size = (1, 3, 128, 128, 128), expand_nested=True, save_graph=True).visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "def binary_dice_score( y_pred: torch.Tensor, y_true: torch.Tensor, threshold: Optional[float] = None, nan_score_on_empty=False, eps: float = 1e-7,) -> float:\n",
    "    # ->; is introduced to get developers to optionally specify the return type of the function.    \n",
    "\n",
    "    # y_pred.shape, y_true.shape => torch.Size([128, 128, 128]) torch.Size([128, 128, 128])\n",
    "    if threshold is not None:\n",
    "        y_pred = (y_pred > threshold).to(y_true.dtype)\n",
    "\n",
    "    intersection = torch.sum(y_pred * y_true).item()\n",
    "    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n",
    "\n",
    "    score = (2.0 * intersection) / (cardinality + eps)\n",
    "\n",
    "    # flags\n",
    "    has_predicted = torch.sum(y_pred) > 0    \n",
    "    has_targets = torch.sum(y_true) > 0 # tensor(True) or tensor(False)\n",
    "\n",
    "    if not has_targets:\n",
    "        if nan_score_on_empty:\n",
    "            score = np.nan\n",
    "        else:\n",
    "            score = float(not has_predicted)\n",
    "    return score\n",
    "\n",
    "\n",
    "def multilabel_dice_score( y_true: torch.Tensor, y_pred: torch.Tensor, threshold=None, eps=1e-7, nan_score_on_empty=False,):\n",
    "    dice_of_lblS = [] # C1....C7 are labels\n",
    "\n",
    "    # y_pred.shape => torch.Size([7, 128, 128, 128])\n",
    "    num_classes = y_pred.size(0)\n",
    "    for class_index in range(num_classes):\n",
    "        dice_of_lbl = binary_dice_score(\n",
    "            y_pred=y_pred[class_index],\n",
    "            y_true=y_true[class_index],\n",
    "            threshold=threshold,\n",
    "            nan_score_on_empty=nan_score_on_empty,\n",
    "            eps=eps,\n",
    "        )\n",
    "        dice_of_lblS.append(dice_of_lbl)\n",
    "\n",
    "    return dice_of_lblS\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    # torch.sigmoid(input) => Computes the expit (also known as the logistic sigmoid function) of the elements of input.\n",
    "    input = torch.sigmoid(input)\n",
    "    # input.shape => torch.Size([4, 7, 128, 128, 128])\n",
    "    \n",
    "    smooth = 1.0\n",
    "    \n",
    "    # flattens the input tensor\n",
    "    iflat = input.view(-1)\n",
    "    # iflat.shape => torch.Size([58720256]) => 4*7*128*128*128 so much dimension\n",
    "\n",
    "    # flattens the target tensor\n",
    "    tflat = target.view(-1)\n",
    "    # tflat.shape => torch.Size([58720256]) => 4*7*128*128*128 so much dimension\n",
    "    \n",
    "    intersection = (iflat * tflat).sum()\n",
    "    # intersection => tensor(82505.0234, device='cuda:0', grad_fn=<SumBackward0>)\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "def bce_dice(input, target, loss_weights=loss_weights): # loss_weights = [1,1] => defined in config section    \n",
    "    # BCEWithLogitsLoss()(input, target) => This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "    # This version is more numerically stable than using a plain Sigmoid followed by a BCELoss.\n",
    "    # BCE => Binary Cross Entropy.\n",
    "    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n",
    "    loss2 = loss_weights[1] * dice_loss(input, target)\n",
    "    return (loss1 + loss2) / sum(loss_weights)\n",
    "\n",
    "criterion = bce_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Valid func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(input, truth, clip=[0, 1]):\n",
    "    # torch.randperm(n, *, ...) => Returns a random permutation of integers from 0 to n - 1.\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    # input.size(0) => batch_size => 4\n",
    "    # indices => tensor([3, 1, 2, 0])\n",
    "    \n",
    "    # shuffling batch in batch of images. \n",
    "    shuffled_input = input[indices]\n",
    "    \n",
    "    # shuffling batch in batch of masks. \n",
    "    shuffled_masks = truth[indices]\n",
    "\n",
    "    # np.random.uniform(low=0.0, high=1.0, size=None) => draw sample(s) from a uniform distribution over the over the half-open interval [low, high).\n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    # lam => 0.632965343426405.\n",
    "\n",
    "    # mixing 'image batch' with 'shuffled image batch' i.e., type of image transformation\n",
    "    input = input * lam + shuffled_input * (1 - lam)\n",
    "    \n",
    "    return input, truth, shuffled_masks, lam\n",
    "\n",
    "\n",
    "def train_func(model, loader_train, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader_train)    \n",
    "    # bar => 0%|                                   | 0/2019 [00:00<?, ?it/s]\n",
    "    # type(bar) => <class 'tqdm.std.tqdm'>    \n",
    "        \n",
    "    for images, gt_masks in bar:\n",
    "        # to reset the gradients of model parameters. \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # .cuda() => transfer a tensor from CPU to GPU.\n",
    "        # .cuda() and .to(device=cuda) are same.\n",
    "        images = images.cuda()\n",
    "        # type(images), images.shape => <class 'torch.Tensor'> torch.Size([4, 3, 128, 128, 128]) => 4 is batch_size\n",
    "\n",
    "        gt_masks = gt_masks.cuda() # gt_masks i.e., get_masks\n",
    "        # type(gt_masks), gt_masks.shape => <class 'torch.Tensor'> torch.Size([4, 7, 128, 128, 128]) => => 4 is batch_size\n",
    "\n",
    "        do_mixup = False\n",
    "        # random.random() => function generates random floating numbers in the range[0.1, 1.0)\n",
    "        if random.random() < p_mixup: # defined in config, p_mixup=0.1\n",
    "            do_mixup = True\n",
    "            images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n",
    "\n",
    "        # torch.amp (automatic mixed precision) => mixed precision tries to match each op to its appropriate datatype.\n",
    "        # autocast() => Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.\n",
    "        with amp.autocast():\n",
    "            # images.shape => torch.Size([4, 3, 128, 128, 128])\n",
    "            logits = model(images)\n",
    "            # logits.shape => torch.Size([4, 7, 128, 128, 128])  \n",
    "            \n",
    "            # calculating loss using masks as targets\n",
    "            loss = criterion(logits, gt_masks)\n",
    "\n",
    "            if do_mixup:\n",
    "                # calculating loss using shuffled masks as targets\n",
    "                loss2 = criterion(logits, gt_masks_sfl)\n",
    "                loss = loss * lam  + loss2 * (1 - lam)\n",
    "                \n",
    "        # loss, loss.item() => \n",
    "        # tensor(0.8821, device='cuda:0', grad_fn=<DivBackward0>), 0.8821219801902771 \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # All gradients produced by scaler.scale(loss).backward() are scaled. \n",
    "        # Each scale is calculated on-the-fly.\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.        \n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        # optimizer's assigned params; parameters which are to be optimized by optimizer.\n",
    "        \n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        bar.set_description(f'smooth loss:{np.mean(train_loss[-30:]):.4f}')\n",
    "        # train_loss[-30:] => take last 30 values of train_loss\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def valid_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    outputs = []\n",
    "    \n",
    "    ths = [0.5]    \n",
    "    # ths = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    \n",
    "    batch_metrics = [[]] * len(ths) # \n",
    "    # if 'len(ths) = 7' then batch_metrics => [[], [], [], [], [], [], []]\n",
    "        \n",
    "    bar = tqdm(loader_valid)\n",
    "    with torch.no_grad():\n",
    "    # disable gradient calculation in this context.\n",
    "    \n",
    "        for images, gt_masks in bar:\n",
    "            images = images.cuda()\n",
    "            gt_masks = gt_masks.cuda()\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, gt_masks)\n",
    "            valid_loss.append(loss.item())\n",
    "            for thi, th in enumerate(ths):\n",
    "                # # .detach(); Returns a new Tensor, detached from the current graph.\n",
    "                # # new Tensor will never require gradient.\n",
    "                # pred = (logits.sigmoid() > th).float().detach()\n",
    "                \n",
    "                for i in range(logits.shape[0]): # logits.shape[0] => batch_size => \n",
    "                    tmp = multilabel_dice_score(\n",
    "                        # .cpu() => copy to cpu\n",
    "                        y_pred=logits[i].sigmoid().cpu(),\n",
    "                        y_true=gt_masks[i].cpu(),\n",
    "                        threshold=th, # 0.5\n",
    "                    )\n",
    "                    # len(tmp) => 7\n",
    "                    # tmp => [0.0, 0.0020704131924468654, 0.0, 0.001438477169412038, 0.0, 0.0, 0.0]\n",
    "                    \n",
    "                    batch_metrics[thi].extend(tmp) # extend previous list\n",
    "            bar.set_description(f'smooth loss:{np.mean(valid_loss[-30:]):.4f}')\n",
    "            \n",
    "    # single value of 'batch_metrics[thi]' list contains score of one patient i.e., (128,128,128).\n",
    "    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n",
    "    print('best threshold:', ths[np.argmax(metrics)], 'best dice of epoch:', np.max(metrics))    \n",
    "\n",
    "    return np.mean(valid_loss), np.max(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = 20, 2\n",
    "# optimizer = optim.AdamW(m.parameters(), lr=init_lr)\n",
    "# scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000)\n",
    "# lrs = []\n",
    "# for epoch in range(1, 1000+1):\n",
    "#     scheduler_cosine.step(epoch-1)\n",
    "#     lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "# plt.plot(range(len(lrs)), lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_run_tr = df_seg[0:4]\n",
    "# df_run_val = df_seg[4:5]\n",
    "def run(fold):\n",
    "\n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "\n",
    "    train_ = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n",
    "    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    dataset_train = SEGDataset(train_, 'train', transform=transforms_train) # train_\n",
    "    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid) # valid_\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = TimmSegModel(backbone, pretrained=True) # True. False; if loading previous model state\n",
    "    model = convert_3d(model)\n",
    "    \n",
    "    # https://pytorch.org/docs/stable/notes/cuda.html    \n",
    "    model = model.to(device) # device = torch.device('cuda')\n",
    "    \n",
    "    # if not first run, load previous model\n",
    "    fold_l = 4\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold_l}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)    \n",
    "    \n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    from_epoch = 0\n",
    "    metric_best = 0.\n",
    "    loss_min = np.inf\n",
    "\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs, eta_min=eta_min)\n",
    "\n",
    "    print(f'Length of dataset_train = {len(dataset_train)}, ', f'Length of dataset_valid = {len(dataset_valid)}')\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        scheduler_cosine.step(epoch-1)\n",
    "\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "        # model training in train_func function\n",
    "        train_loss = train_func(model, loader_train, optimizer, scaler)\n",
    "        valid_loss, metric = valid_func(model, loader_valid)\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n",
    "        print(content)\n",
    "        with open(log_file, 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        if metric > metric_best:\n",
    "            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            metric_best = metric\n",
    "\n",
    "#         # Save Last\n",
    "#         if not DEBUG:\n",
    "#             torch.save(\n",
    "#                 {\n",
    "#                     'epoch': epoch,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "#                     'score_best': metric_best,\n",
    "#                 },\n",
    "#                 model_file.replace('_best', '_last')\n",
    "#             )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "#     _= gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.version.cuda)\n",
    "# print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-23 11:24:52,642 - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21ft1k-d7dafa41.pth)\n",
      "Length of dataset_train = 69,  Length of dataset_valid = 18\n",
      "Thu Feb 23 11:24:53 2023 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0384: 100%|███████████████████████| 23/23 [01:04<00:00,  2.81s/it]\n",
      "smooth loss:0.0343: 100%|█████████████████████████| 6/6 [00:13<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9307108726825309\n",
      "Thu Feb 23 11:26:11 2023 Fold 0, Epoch 1, lr: 0.0003000, train loss: 0.03842, valid loss: 0.03425, metric: 0.930711.\n",
      "metric_best (0.000000 --> 0.930711). Saving model ...\n",
      "Thu Feb 23 11:26:12 2023 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0304: 100%|███████████████████████| 23/23 [00:25<00:00,  1.11s/it]\n",
      "smooth loss:0.0342: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9300611238882839\n",
      "Thu Feb 23 11:26:44 2023 Fold 0, Epoch 2, lr: 0.0003000, train loss: 0.03041, valid loss: 0.03424, metric: 0.930061.\n",
      "Thu Feb 23 11:26:44 2023 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0302: 100%|███████████████████████| 23/23 [00:25<00:00,  1.13s/it]\n",
      "smooth loss:0.0341: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9308472615778829\n",
      "Thu Feb 23 11:27:17 2023 Fold 0, Epoch 3, lr: 0.0003000, train loss: 0.03016, valid loss: 0.03409, metric: 0.930847.\n",
      "metric_best (0.930711 --> 0.930847). Saving model ...\n",
      "Thu Feb 23 11:27:17 2023 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0470: 100%|███████████████████████| 23/23 [00:25<00:00,  1.12s/it]\n",
      "smooth loss:0.0350: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9273618513745848\n",
      "Thu Feb 23 11:27:50 2023 Fold 0, Epoch 4, lr: 0.0003000, train loss: 0.04697, valid loss: 0.03498, metric: 0.927362.\n",
      "Thu Feb 23 11:27:50 2023 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0344: 100%|███████████████████████| 23/23 [00:25<00:00,  1.10s/it]\n",
      "smooth loss:0.0352: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9273315913554759\n",
      "Thu Feb 23 11:28:22 2023 Fold 0, Epoch 5, lr: 0.0003000, train loss: 0.03444, valid loss: 0.03517, metric: 0.927332.\n",
      "Thu Feb 23 11:28:22 2023 Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0290: 100%|███████████████████████| 23/23 [00:24<00:00,  1.07s/it]\n",
      "smooth loss:0.0347: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9290590532722638\n",
      "Thu Feb 23 11:28:53 2023 Fold 0, Epoch 6, lr: 0.0003000, train loss: 0.02897, valid loss: 0.03474, metric: 0.929059.\n",
      "Thu Feb 23 11:28:53 2023 Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0324: 100%|███████████████████████| 23/23 [00:24<00:00,  1.09s/it]\n",
      "smooth loss:0.0352: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.927584668033893\n",
      "Thu Feb 23 11:29:25 2023 Fold 0, Epoch 7, lr: 0.0003000, train loss: 0.03239, valid loss: 0.03524, metric: 0.927585.\n",
      "Thu Feb 23 11:29:25 2023 Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0412: 100%|███████████████████████| 23/23 [00:24<00:00,  1.08s/it]\n",
      "smooth loss:0.0363: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9247637364760257\n",
      "Thu Feb 23 11:29:56 2023 Fold 0, Epoch 8, lr: 0.0003000, train loss: 0.04120, valid loss: 0.03632, metric: 0.924764.\n",
      "Thu Feb 23 11:29:56 2023 Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0356: 100%|███████████████████████| 23/23 [00:25<00:00,  1.09s/it]\n",
      "smooth loss:0.0352: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9273894142530573\n",
      "Thu Feb 23 11:30:28 2023 Fold 0, Epoch 9, lr: 0.0003000, train loss: 0.03558, valid loss: 0.03524, metric: 0.927389.\n",
      "Thu Feb 23 11:30:28 2023 Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0695: 100%|███████████████████████| 23/23 [00:25<00:00,  1.09s/it]\n",
      "smooth loss:0.0367: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.9240821711293389\n",
      "Thu Feb 23 11:31:00 2023 Fold 0, Epoch 10, lr: 0.0002999, train loss: 0.06951, valid loss: 0.03667, metric: 0.924082.\n",
      "Thu Feb 23 11:31:00 2023 Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0289: 100%|███████████████████████| 23/23 [00:24<00:00,  1.08s/it]\n",
      "smooth loss:0.0362: 100%|█████████████████████████| 6/6 [00:06<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold: 0.5 best dice of epoch: 0.925965181133139\n",
      "Thu Feb 23 11:31:31 2023 Fold 0, Epoch 11, lr: 0.0002999, train loss: 0.02893, valid loss: 0.03616, metric: 0.925965.\n",
      "Thu Feb 23 11:31:31 2023 Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smooth loss:0.0279:  17%|████▏                   | 4/23 [00:07<00:33,  1.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_715283/1118381438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# during first execution do not run all folds in continuity. run every fold after kernel restart.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# during second execution of this cell, choose those weights which performed well from all folds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# run(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# run(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_715283/4099378241.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# model training in train_func function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_715283/282726592.py\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(model, loader_train, optimizer, scaler)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Scales loss.  Calls backward() on scaled loss to create scaled gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# All gradients produced by scaler.scale(loss).backward() are scaled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Each scale is calculated on-the-fly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base_2/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base_2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# execute this cell twice.\n",
    "# during first execution do not run all folds in continuity. run every fold after kernel restart.\n",
    "# during second execution of this cell, choose those weights which performed well from all folds.\n",
    "run(0)\n",
    "# run(1) \n",
    "# run(2)\n",
    "# run(3)\n",
    "# run(4)\n",
    "\n",
    "# maximum difference of 0.10 between \"smooth loss train\" and \"smooth loss valid\" is OK, otherwise reduce the lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

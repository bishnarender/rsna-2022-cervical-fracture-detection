{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:36.367300Z",
     "iopub.status.busy": "2022-10-30T05:05:36.366529Z",
     "iopub.status.idle": "2022-10-30T05:05:36.383599Z",
     "shell.execute_reply": "2022-10-30T05:05:36.382570Z",
     "shell.execute_reply.started": "2022-10-30T05:05:36.367257Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "from timm0412 import timm as timm # timm0412 means timm v0.4.12\n",
    "# import timm\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.__version__#, timm4smp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai.transforms as transforms\n",
    "from monai.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install dask-cuda\n",
    "# conda install -c rapidsai -c conda-forge -c nvidia cuml=21.10\n",
    "import cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses a RAID folder if it is available (DGX), if not it uses the /tmp folder for DASK workspace\n",
    "if os.path.exists('/raid'):\n",
    "    local_directory = '/raid'\n",
    "else:    \n",
    "    local_directory = '/tmp'\n",
    "    \n",
    "dask_workdir = os.path.join(local_directory, 'dask-workdir')    \n",
    "print('Dask dir:', dask_workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a clean worker space for Dask\n",
    "if os.path.isdir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir) # to Delete Non-Empty Directory\n",
    "os.mkdir(dask_workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Dask is often used in situations where the data are too big to fit in memory. In these cases the data are split into chunks or partitions. Each task is computed on the chunk and then the results are aggregated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For chunked data, if each worker is able to comfortably hold one data chunk in memory and do some computation on that data, then the number of chunks should be a multiple of the number of workers. This ensures that there is always enough work for a worker to do.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster manager class LocalCUDACluster, this cluster manager is optimised for a single piece of hardware.\n",
    "# create a Dask-CUDA cluster using all available GPUs\n",
    "cluster = LocalCUDACluster(#dashboard_address=':8800',\n",
    "                        CUDA_VISIBLE_DEVICES = CUDA_VISIBLE_DEVICES,\n",
    "                        rmm_pool_size=None, # no RMM pool is initialized.\n",
    "                        device_memory_limit=7516192768, # spill to host memory when 7GB of 8GB (GPU) is reached\n",
    "                        local_directory = dask_workdir) # launches a \"scheduler\" and workers locally\n",
    "\n",
    "# https://dask-cuda.readthedocs.io/en/stable/api.html#dask_cuda.LocalCUDACluster\n",
    "# https://docs.rapids.ai/api/dask-cuda/stable/api.html\n",
    "# https://docs.dask.org/en/latest/deploying.html\n",
    "\n",
    "\n",
    "# connect a Dask.distributed Client to Dask-CUDA cluster:\n",
    "client = Client(cluster) #  client = Client() set up local cluster on your laptop\n",
    "# when we create a Client object it registers itself as the default Dask scheduler. -\n",
    "# - all .compute() methods will automatically start using the distributed system.\n",
    "\n",
    "\n",
    "# https://distributed.dask.org/en/latest/quickstart.html\n",
    "# 7GB = 7516192768 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize RMM pool on ALL workers\n",
    "# def _rmm_pool():\n",
    "#     rmm.reinitialize(\n",
    "#         pool_allocator=True,\n",
    "#         initial_pool_size=None, # Use default size\n",
    "#     )\n",
    "\n",
    "# # https://medium.com/rapids-ai/reading-larger-than-memory-csvs-with-rapids-and-dask-e6e27dfa6c0f\n",
    "# # https://github.com/rapidsai/rmm\n",
    "# # https://docs.rapids.ai/api/rmm/stable/basics.html\n",
    "\n",
    "\n",
    "# client.run(_rmm_pool)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses RAM memory when cudf spills over gpu memory\n",
    "client.run(cudf.set_allocator, \"managed\")  # uses managed memory instead of \"default\"\n",
    "# https://github.com/rapidsai/cudf/blob/4e66281f48c55735edb4b610e0f859ee2de32a75/python/cudf/cudf/utils/utils.py#L193\n",
    "# https://distributed.dask.org/en/stable/api.html#distributed.Client.run\n",
    "\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:38.244360Z",
     "iopub.status.busy": "2022-10-30T05:05:38.243965Z",
     "iopub.status.idle": "2022-10-30T05:05:38.251074Z",
     "shell.execute_reply": "2022-10-30T05:05:38.250013Z",
     "shell.execute_reply.started": "2022-10-30T05:05:38.244329Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "image_size_seg = (128, 128, 128)\n",
    "msk_size = image_size_seg[0]\n",
    "image_size_cls = 224\n",
    "n_slice_per_c = 15\n",
    "n_ch = 5\n",
    "\n",
    "R = Resize([2, 224, 6], mode=\"trilinear\") # monai => Resize => 'trilinear' for 3d output\n",
    "\n",
    "batch_size_seg = 1\n",
    "num_workers = 12 # 2\n",
    "model_dir_seg = './kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:38.550565Z",
     "iopub.status.busy": "2022-10-30T05:05:38.550234Z",
     "iopub.status.idle": "2022-10-30T05:05:38.593693Z",
     "shell.execute_reply": "2022-10-30T05:05:38.592753Z",
     "shell.execute_reply.started": "2022-10-30T05:05:38.550535Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'train_seg.csv'))# train.csv \n",
    "    df = pd.DataFrame({\n",
    "        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n",
    "    })\n",
    "    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:39.722018Z",
     "iopub.status.busy": "2022-10-30T05:05:39.720643Z",
     "iopub.status.idle": "2022-10-30T05:05:39.733841Z",
     "shell.execute_reply": "2022-10-30T05:05:39.732824Z",
     "shell.execute_reply.started": "2022-10-30T05:05:39.721971Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = cv2.resize(data, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    \n",
    "    n_scans = len(t_paths)\n",
    "#     print(n_scans)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "\n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    images = np.stack(images, 0) # -1\n",
    "    \n",
    "    images = images - np.min(images)\n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    return images, indices\n",
    "\n",
    "\n",
    "class SegTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        image, index = load_dicom_line_par(row.image_folder)\n",
    "        if image.ndim < 4:\n",
    "            image = np.expand_dims(image, axis=0).repeat(3, axis=0) # to 3ch\n",
    "\n",
    "        image = image / 255.\n",
    "        return torch.tensor(image).float(), index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:39.884168Z",
     "iopub.status.busy": "2022-10-30T05:05:39.883882Z",
     "iopub.status.idle": "2022-10-30T05:05:39.890695Z",
     "shell.execute_reply": "2022-10-30T05:05:39.889521Z",
     "shell.execute_reply.started": "2022-10-30T05:05:39.884141Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset_seg = SegTestDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:41.650857Z",
     "iopub.status.busy": "2022-10-30T05:05:41.650555Z",
     "iopub.status.idle": "2022-10-30T05:05:41.657196Z",
     "shell.execute_reply": "2022-10-30T05:05:41.655984Z",
     "shell.execute_reply.started": "2022-10-30T05:05:41.650829Z"
    }
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     rcParams['figure.figsize'] = 20,8\n",
    "#     for i in range(1):\n",
    "#         f, axarr = plt.subplots(1,4)\n",
    "#         for p in range(4):\n",
    "#             idx = i*4+p\n",
    "#             img = dataset_seg[idx]\n",
    "#             # img.shape => torch.Size([3, 128, 128, 128])\n",
    "\n",
    "#             img = img[:, 60, :, :] # picking 60th dcm image of a patient/folder.\n",
    "#             # img.shape => torch.Size([3, 128, 128])\n",
    "            \n",
    "#             axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())\n",
    "#             # img.transpose(0, 1).shape => torch.Size([128, 3, 128])\n",
    "#             # img.transpose(0, 1).transpose(1,2).shape => torch.Size([128, 128, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:58.414747Z",
     "iopub.status.busy": "2022-10-30T05:05:58.414309Z",
     "iopub.status.idle": "2022-10-30T05:05:58.459664Z",
     "shell.execute_reply": "2022-10-30T05:05:58.458503Z",
     "shell.execute_reply.started": "2022-10-30T05:05:58.414692Z"
    }
   },
   "outputs": [],
   "source": [
    "from conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        \n",
    "        # module => BatchNorm2d\n",
    "        # affine â€“ a boolean value that when set to True, this module has learnable affine parameters.\n",
    "        # parameters weight and bias are only defined if the argument affine is set to True.\n",
    "        if module.affine:\n",
    "            # torch.no_grad() temporarily sets all of the requires_grad flags to false.\n",
    "            # 'requires_grad' flag is set then model will compute gradient w.r.t to parameter.\n",
    "            with torch.no_grad():\n",
    "            # with => ensures that resource is \"cleaned up\" when the code that uses it finishes running, even if exceptions are thrown.\n",
    "            # with torch.no_grad() => disable gradient calculation in this context.\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module( name, convert_3d(child) )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model( \n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [160, 64, 48, 24, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "                attention_type='scse',\n",
    "            )\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        enc_features = self.encoder(x)[:n_blocks]\n",
    "        f, d = enc_features[0].shape[0], enc_features[0].device\n",
    "        a = [24,48,64,160]\n",
    "        b = [63,31,15,7]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],1,b[i],b[i]), device=d).float()), dim=2) for i, feat in enumerate(enc_features)]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,1,b[i]), device=d).float()), dim=3) for i, feat in enumerate(enc_features)]      \n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,b[i]+1,1), device=d).float()), dim=4) for i, feat in enumerate(enc_features)]           \n",
    "        global_features = [0] + enc_features\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:06:02.248200Z",
     "iopub.status.busy": "2022-10-30T05:06:02.247561Z",
     "iopub.status.idle": "2022-10-30T05:06:18.464686Z",
     "shell.execute_reply": "2022-10-30T05:06:18.463650Z",
     "shell.execute_reply.started": "2022-10-30T05:06:02.248158Z"
    }
   },
   "outputs": [],
   "source": [
    "models_seg = []\n",
    "\n",
    "kernel_type = 'timm3d_effv2_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "\n",
    "n_blocks = 4\n",
    "for fold in range(5): # 5\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    # sd.keys() =>\n",
    "    # odict_keys(['encoder.conv1.0.weight', 'encoder.conv1.1.weight', 'encoder.conv1.1.bias', \n",
    "    # 'encoder.conv1.1.running_mean', 'encoder.conv1.1.running_var', \n",
    "    # ..................  'segmentation_head.weight', 'segmentation_head.bias'])\n",
    "        \n",
    "    # use this if 'model file .pth' is stored with extra information\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "        \n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    # changed odict to dict\n",
    "    # sd.keys() => \n",
    "    # dict_keys(['encoder.conv1.0.weight', 'encoder.conv1.1.weight', 'encoder.conv1.1.bias', \n",
    "    # 'encoder.conv1.1.running_mean', 'encoder.conv1.1.running_var', \n",
    "    # ..................  'segmentation_head.weight', 'segmentation_head.bias'])    \n",
    "    \n",
    "    model.load_state_dict(sd, strict=True) # strict=True\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "len(models_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_seg = SegTestDataset(df[6:7])\n",
    "# loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:11:35.770395Z",
     "iopub.status.busy": "2022-10-30T05:11:35.770051Z",
     "iopub.status.idle": "2022-10-30T05:11:43.226734Z",
     "shell.execute_reply": "2022-10-30T05:11:43.225225Z",
     "shell.execute_reply.started": "2022-10-30T05:11:35.770366Z"
    }
   },
   "outputs": [],
   "source": [
    "# # plotting some prediction\n",
    "\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "# bar = tqdm(loader_seg)\n",
    "# # bar => 0%|                                   | 0/2019 [00:00<?, ?it/s]\n",
    "# # type(bar) => <class 'tqdm.std.tqdm'>\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     f, axarr = plt.subplots(1,4)    \n",
    "    \n",
    "#     for batch_id, images in enumerate(bar):\n",
    "#         # images.shape => torch.Size([1, 3, 128, 128, 128])\n",
    "        \n",
    "#         images = images.cuda()\n",
    "\n",
    "#         # SEG\n",
    "#         pred_masks = []\n",
    "#         for model in models_seg:\n",
    "#             pmask = model(images).sigmoid()\n",
    "#             pred_masks.append(pmask)\n",
    "            \n",
    "#             # pred_masks[0].shape => torch.Size([1, 7, 128, 128, 128])\n",
    "#             # torch.stack(pred_masks, 0).shape) => torch.Size([5, 1, 7, 128, 128, 128])\n",
    "# #         pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()            \n",
    "#         pred_masks = torch.stack(pred_masks, 0).mean(dim = 0).cpu() #  taking mean of all 5 predictions\n",
    "#             # pred_masks.shape => (1, 7, 128, 128, 128)\n",
    "        \n",
    "#         images = images.cpu()\n",
    "#         images = images.squeeze() # numpy like squeeze on torch tensor.        \n",
    "#         # images.shape => torch.Size([3, 128, 128, 128])               \n",
    "        \n",
    "#         # converting mask_values to 0.0 and 1.0\n",
    "#         pred_masks = (pred_masks>0.5).float().squeeze()\n",
    "#             # pred_masks.shape => torch.Size([7, 128, 128, 128])         \n",
    "        \n",
    "#         c = 0\n",
    "#         for i in range(0,2):\n",
    "#             if i==0:\n",
    "#                 img = images[:, 60, :, :].detach().clone() # checking 60th dcm image for a particular patient       \n",
    "#                 masks = pred_masks[:, 60, :, :].detach().clone() # checking corresponding 60th slice \n",
    "#                     # images.shape, pred_masks.shape => (3, 128, 128) (7, 128, 128)        \n",
    "#             else:\n",
    "#                 img = images[:, :, :, 60].detach().clone()\n",
    "#                 masks = pred_masks[:, :, :, 60].detach().clone()          \n",
    "\n",
    "#             # merging 7 channels in order to reduce to 3\n",
    "#             masks[0] = masks[0] + masks[3] + masks[6] # merging C1, C4 and C7\n",
    "#             masks[1] = masks[1] + masks[4] # merging C2, C5\n",
    "#             masks[2] = masks[2] + masks[5] # merging C3, C6\n",
    "\n",
    "#             masks = masks[:3] # selecting only 3 sequence / channels out of 7.        \n",
    "\n",
    "#             axarr[c].imshow(img.transpose(0, 1).transpose(1,2)) # squeeze(); removes axes of length one.\n",
    "#             axarr[c+1].imshow(masks.transpose(0, 1).transpose(1,2))         \n",
    "#             c += 2\n",
    "        \n",
    "#         del img, masks,# images#, pred_masks\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "#         _ = gc.collect()\n",
    "        \n",
    "\n",
    "#         break          \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Mask and Crop Slices on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bone(msk, cid, t_paths, cropped_images, index, inst_id):\n",
    "    sema.acquire() #  threading topic    \n",
    "    n_scans = len(t_paths)\n",
    "    bone = []\n",
    "    try:\n",
    "        msk_b = msk[cid] > 0.2 # as best_threshold in train_1_efficient.ipynb is 0.2.\n",
    "            # msk_b.shape => (128, 128, 128)\n",
    "            # msk_b => \n",
    "            # [[[False False False ... False False False]        \n",
    "            #   ...\n",
    "            #   ...\n",
    "            #   [False False False ... False False False]\n",
    "            #   [False False False ... False False False]]]          \n",
    "          \n",
    "        msk_c = msk[cid] > 0.02\n",
    "                \n",
    "            # msk_b.sum(axis = 1).shape => (128, 128)\n",
    "            # msk_b.sum(1).sum(1).shape =>  (128,) \n",
    "            # msk_b.sum(1).sum(1) => \n",
    "            # [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
    "            #   18  75 157 239 307 382 485 592 655 663 688 809 810 601 323 100  36  10\n",
    "            #    0   0   1   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "            #    ...\n",
    "            #    ...\n",
    "            #    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "            #    0   0]            \n",
    "            # np.where(msk_b.sum(1).sum(1) > 0) => \n",
    "            # (array([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
    "            #        34, 35, 38, 39]),)              \n",
    "\n",
    "        # finding indices of three coordinate axes of mask_cuboid (128,128,128) where data is present.\n",
    "        # (128,128,128) => (slices, W, H)\n",
    "        y = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "        x = np.where(msk_b.sum(0).sum(1) > 0)[0]        \n",
    "        z = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            y = np.where(msk_c.sum(0).sum(0) > 0)[0]            \n",
    "            x = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(1).sum(1) > 0)[0]            \n",
    "\n",
    "        # msk.shape => (7, 128, 128, 128)\n",
    "        y1, y2 = max(0, y[0]), min(msk.shape[2]-1, y[-1])        \n",
    "        x1, x2 = max(0, x[0]), min(msk.shape[3]-1, x[-1])\n",
    "        z1, z2 = max(2, z[0]), min(msk.shape[1]-3, z[-1])\n",
    "        \n",
    "#             # z1, z2 => 26 98\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans) # msk_size = 128, defined in config section\n",
    "#             # from BODMAS rule => zz1, zz2 => 49 186\n",
    "#             # (z1 / msk_size) => proportion of z1-coordinate over index length 128 (msk_size).\n",
    "#             # (z1 / msk_size) * n_scans => z1-coordinate over index length 'n_scans'.\n",
    "            \n",
    "        ## return 15 (n_slice_per_c) evenly spaced indexes.\n",
    "        inds_ = np.linspace(z1 ,z2, n_slice_per_c).astype(int) \n",
    "            # inds_ = np.linspace(26, 98, 128) => [26 31 36 41 46 51 56 61 66 71 76 81 86 91 97]        \n",
    "            # np.linspace(start, stop, num_of_samples_to_generate, ...) => Return evenly spaced numbers over a specified interval. \n",
    "            \n",
    "        inds = np.linspace(zz1 ,zz2, n_slice_per_c).astype(int)\n",
    "            # inds => [ 49  58  68  78  87  97 107 117 126 136 146 155 165 175 185]\n",
    "\n",
    "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):        \n",
    "#         for ind_ in inds_:\n",
    "#             k = index[0][ind_]\n",
    "            images = []\n",
    "#             for i in range(-n_ch//2+1, n_ch//2+1): # n_ch = 5, defined in config \n",
    "            for i in [-2,-1,0,1,2]:\n",
    "                # for i in (-2, 3):\n",
    "                # 5//2 = 2, -5//2 = -3 (1 extra in -ve)\n",
    "                try:\n",
    "#                     dicom = pydicom.read_file(t_paths[k+i])\n",
    "                        # ind_ = 26 read 24, 25, 26, 27, 28 dicom files \n",
    "                    dicom = pydicom.read_file(t_paths[ind+i])                     \n",
    "                    images.append(dicom.pixel_array)\n",
    "                except:\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "                    \n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "                # data.shape => (512, 512, 5)\n",
    "            \n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)  \n",
    "                # prior to any type of transformation(resize, augmentation etc.) convert data to uint8.\n",
    "            \n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "            \n",
    "            \n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "                # data.shape => (96, 172, 5)\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "                # data.shape => (224, 224, 5)                     \n",
    "\n",
    "            # sticking center mask at lask such that area around it, will be targeted.\n",
    "            msk_this = msk[cid, ind_, :, :] # ind_ = 26 \n",
    "            \n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "                # msk_this[16:40, 23:66].shape => (24, 43)                                                \n",
    "            msk_this = (msk_this * 255).astype(np.uint8)\n",
    "                # prior to any type of transformation(resize, augmentation etc.) convert data to uint8.\n",
    "                # np.unique(msk_this).astype(np.uint8)) =>             \n",
    "                # [  0   1   2   4   6   8  11  14  21  23  48  49  50  61  69  72  73  81\n",
    "                #  107 109 113 116 121 128 143 144 147 161 181 192 204 207 209 214 232 236\n",
    "                #  240 241 245 247 250 252 253 254 255]                                \n",
    "                \n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)            \n",
    "                # msk_this.shape => (224, 224)\n",
    "\n",
    "            # appending mask at as 6th channel of data. \n",
    "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)            \n",
    "                 # data.shape => (224, 224, 6)\n",
    "\n",
    "            bone.append(torch.tensor(data))\n",
    "\n",
    "    except:\n",
    "        for sid in range(n_slice_per_c):\n",
    "            bone.append(torch.zeros((image_size_cls, image_size_cls, n_ch+1)).to(torch.uint8))\n",
    "        \n",
    "    cropped_images[cid] = torch.stack(bone, 0)\n",
    "        # cropped_images[cid].shape => torch.Size([15, 224, 224, 6])    \n",
    "        \n",
    "    ### using local cache\n",
    "    image_file = os.path.join(data_dir, f'numpy_2/{inst_id}_{cid+1}.npy')\n",
    "    np.save(image_file, cropped_images[cid])\n",
    "    \n",
    "    sema.release() #  threading topic    \n",
    "\n",
    "def load_cropped_images(msk, image_folder, index, inst_id, n_ch=n_ch):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    \n",
    "    for cid in range(7): # 7\n",
    "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images, index, inst_id))\n",
    "        threads[cid].start()\n",
    "    for cid in range(7):\n",
    "        threads[cid].join()\n",
    "\n",
    "        # torch.cat(tensors, dim=0,..) => Concatenates the given sequence of seq tensors in the given dimension. \n",
    "        # torch.cat(cropped_images, 0).shape => torch.Size([105, 224, 224, 6])    \n",
    "    return torch.cat(cropped_images, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sema = threading.Semaphore(value=12) # value => setting number of threads\n",
    "dataset_seg = SegTestDataset(df) # df[1:2] df[3:4] \n",
    "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bar = tqdm(loader_seg)\n",
    "with torch.no_grad():\n",
    "    for batch_id, (images, indices) in enumerate(bar):\n",
    "        indices = indices.numpy()\n",
    "        images = images.cuda()\n",
    "        # SEG\n",
    "        pred_masks = []\n",
    "        for model in models_seg:            \n",
    "            pmask = model(images).sigmoid()\n",
    "                # torch.unique(model(images)) => \n",
    "                # tensor([-191.6912, -190.9502, -190.5765,  ...,   36.2534,   36.5492,\n",
    "                #           36.8366], device='cuda:0')\n",
    "\n",
    "            pred_masks.append(pmask)\n",
    "        pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n",
    "            # pred_masks.shape => (1, 7, 128, 128, 128)\n",
    "        \n",
    "        # Build cls input\n",
    "        cls_inp = []\n",
    "        threads = [None] * 7\n",
    "        cropped_images = [None] * 7\n",
    "\n",
    "        for i in range(pred_masks.shape[0]):\n",
    "            \n",
    "             # batch_id, batch_size_seg, i => 0, 1, 0\n",
    "            row = df.iloc[batch_id*batch_size_seg+i] # df[1:2] df[3:4] \n",
    "\n",
    "            cropped_images = load_cropped_images(pred_masks[i], row.image_folder, indices, row.StudyInstanceUID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [113 117 122 126 131 135 140 144 149 153 158 162 167 171 176]\n",
    "# [114, 118, 122, 126, 130, 134, 138, 146, 150, 154, 157, 161, 165, 169, 177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cropped_images.shape)\n",
    "# # print(torch.min(cropped_images), torch.max(cropped_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all Cid (for one patient) one by one in order to verify that images are showing correctly or not -\n",
    "# - otherwise adjust numeric value in 'msk[cid] > 0.1' in above function.\n",
    "# plotter Cid = 2\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for i in range(17,27):\n",
    "    fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "    for j in range(6):\n",
    "        arr[j].imshow(cropped_images[i][:,:,j])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter Cid = 1\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for i in range(1,11):\n",
    "    fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "    for j in range(6):\n",
    "        arr[j].imshow(cropped_images[i][:,:,j])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter Cid = 7\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for i in range(65,75):\n",
    "    fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "    for j in range(6):\n",
    "        arr[j].imshow(cropped_images[i][:,:,j])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter Cid = 7\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for i in range(95,105):\n",
    "    fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "    for j in range(6):\n",
    "        arr[j].imshow(cropped_images[i][:,:,j])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter Cid = 7\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "for i in range(80,90):\n",
    "    fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "    for j in range(6):\n",
    "        arr[j].imshow(cropped_images[i][:,:,j])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

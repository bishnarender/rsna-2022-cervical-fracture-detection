{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:36.367300Z",
     "iopub.status.busy": "2022-10-30T05:05:36.366529Z",
     "iopub.status.idle": "2022-10-30T05:05:36.383599Z",
     "shell.execute_reply": "2022-10-30T05:05:36.382570Z",
     "shell.execute_reply.started": "2022-10-30T05:05:36.367257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na/miniconda3/envs/base_2/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/na/miniconda3/envs/base_2/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os, gc, ast, cv2, time, timm, pickle, random, pydicom\n",
    "\n",
    "from timm0412 import timm as timm4smp \n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:38.244360Z",
     "iopub.status.busy": "2022-10-30T05:05:38.243965Z",
     "iopub.status.idle": "2022-10-30T05:05:38.251074Z",
     "shell.execute_reply": "2022-10-30T05:05:38.250013Z",
     "shell.execute_reply.started": "2022-10-30T05:05:38.244329Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "image_size_seg = (128, 128, 128)\n",
    "msk_size = image_size_seg[0]\n",
    "image_size_cls = 224\n",
    "n_slice_per_c = 15\n",
    "n_ch = 5\n",
    "\n",
    "batch_size_seg = 1\n",
    "num_workers = 2\n",
    "model_dir_seg = './kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:38.550565Z",
     "iopub.status.busy": "2022-10-30T05:05:38.550234Z",
     "iopub.status.idle": "2022-10-30T05:05:38.593693Z",
     "shell.execute_reply": "2022-10-30T05:05:38.592753Z",
     "shell.execute_reply.started": "2022-10-30T05:05:38.550535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>image_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.22327</td>\n",
       "      <td>./test_images/1.2.826.0.1.3680043.22327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.25399</td>\n",
       "      <td>./test_images/1.2.826.0.1.3680043.25399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.5876</td>\n",
       "      <td>./test_images/1.2.826.0.1.3680043.5876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID                             image_folder\n",
       "0  1.2.826.0.1.3680043.22327  ./test_images/1.2.826.0.1.3680043.22327\n",
       "1  1.2.826.0.1.3680043.25399  ./test_images/1.2.826.0.1.3680043.25399\n",
       "2   1.2.826.0.1.3680043.5876   ./test_images/1.2.826.0.1.3680043.5876"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'train.csv'))[0:100]\n",
    "    df = pd.DataFrame({\n",
    "        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n",
    "    })\n",
    "    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    if df.iloc[0].row_id == '1.2.826.0.1.3680043.10197_C1':\n",
    "        # test_images and test.csv are inconsistent in the dev dataset, fixing labels for the dev run.\n",
    "        df = pd.DataFrame({\"row_id\": ['1.2.826.0.1.3680043.22327_C1', '1.2.826.0.1.3680043.25399_C1', '1.2.826.0.1.3680043.5876_C1'],\n",
    "                               \"StudyInstanceUID\": ['1.2.826.0.1.3680043.22327', '1.2.826.0.1.3680043.25399', '1.2.826.0.1.3680043.5876'],\n",
    "                               \"prediction_type\": [\"C1\", \"C1\", \"C1\"]}\n",
    "                         )\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n",
    "    })\n",
    "    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'test_images', x))\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:39.722018Z",
     "iopub.status.busy": "2022-10-30T05:05:39.720643Z",
     "iopub.status.idle": "2022-10-30T05:05:39.733841Z",
     "shell.execute_reply": "2022-10-30T05:05:39.732824Z",
     "shell.execute_reply.started": "2022-10-30T05:05:39.721971Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = cv2.resize(data, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "\n",
    "    n_scans = len(t_paths)\n",
    "#     print(n_scans)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "\n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    images = np.stack(images, 0) # -1\n",
    "    \n",
    "    images = images - np.min(images)\n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    return images, indices\n",
    "\n",
    "\n",
    "class SegTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        image, index = load_dicom_line_par(row.image_folder)\n",
    "        if image.ndim < 4:\n",
    "            image = np.expand_dims(image, axis=0).repeat(3, axis=0) # to 3ch\n",
    "\n",
    "        image = image / 255.\n",
    "        return torch.tensor(image).float(), index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:05:58.414747Z",
     "iopub.status.busy": "2022-10-30T05:05:58.414309Z",
     "iopub.status.idle": "2022-10-30T05:05:58.459664Z",
     "shell.execute_reply": "2022-10-30T05:05:58.458503Z",
     "shell.execute_reply.started": "2022-10-30T05:05:58.414692Z"
    }
   },
   "outputs": [],
   "source": [
    "from conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        \n",
    "        # module => BatchNorm2d\n",
    "        # affine – a boolean value that when set to True, this module has learnable affine parameters.\n",
    "        # parameters weight and bias are only defined if the argument affine is set to True.\n",
    "        if module.affine:\n",
    "            # torch.no_grad() temporarily sets all of the requires_grad flags to false.\n",
    "            # 'requires_grad' flag is set then model will compute gradient w.r.t to parameter.\n",
    "            with torch.no_grad():\n",
    "            # with => ensures that resource is \"cleaned up\" when the code that uses it finishes running, even if exceptions are thrown.\n",
    "            # with torch.no_grad() => disable gradient calculation in this context.\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module( name, convert_3d(child) )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm4smp.create_model( # timm4smp\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [160, 64, 48, 24, 16] \n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "                attention_type='scse',\n",
    "            )\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        enc_features = self.encoder(x)[:n_blocks]\n",
    "        f, d = enc_features[0].shape[0], enc_features[0].device\n",
    "        a = [24,48,64,160]\n",
    "        b = [63,31,15,7]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],1,b[i],b[i]), device=d).float()), dim=2) for i, feat in enumerate(enc_features)]\n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,1,b[i]), device=d).float()), dim=3) for i, feat in enumerate(enc_features)]      \n",
    "        enc_features = [torch.cat((feat, torch.zeros((f,a[i],b[i]+1,b[i]+1,1), device=d).float()), dim=4) for i, feat in enumerate(enc_features)]           \n",
    "        global_features = [0] + enc_features\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "  \n",
    "    \n",
    "    \n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        # self.encoder.default_cfg =>\n",
    "        # {'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21ft1k-d7dafa41.pth', \n",
    "        # 'num_classes': 1000, 'input_size': (3, 300, 300), 'pool_size': (10, 10), 'crop_pct': 1.0, 'interpolation': \n",
    "        # 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'first_conv': 'conv_stem', 'classifier': 'classifier', \n",
    "        # 'test_input_size': (3, 384, 384), 'architecture': 'tf_efficientnetv2_s_in21ft1k'}        \n",
    "\n",
    "\n",
    "        \n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "                # (conv_head): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False) \n",
    "                # self.encoder.conv_head => Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)  \n",
    "                # self.encoder.conv_head.out_channels => 1280\n",
    "                \n",
    "                # nn.Identity() => Identity()\n",
    "                # self.encoder.classifier => Linear(in_features=1280, out_features=1, bias=True)  \n",
    "            # replace the last classifier layer with identity layer.\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n",
    "        # x.shape => torch.Size([2, 15, 6, 224, 224])\n",
    "        \n",
    "        bs = x.shape[0]\n",
    "        # Tensor.view(*shape) => Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        x = x.view(bs * n_slice_per_c, in_chans, image_size, image_size)\n",
    "            # x.shape => torch.Size([30, 6, 224, 224])\n",
    "        \n",
    "        feat = self.encoder(x)        \n",
    "\n",
    "            # feat.shape => torch.Size([30, 1280])        \n",
    "        feat = feat.view(bs, n_slice_per_c, -1)\n",
    "            # feat.shape => torch.Size([2, 15, 1280])\n",
    "        \n",
    "        feat, _ = self.lstm(feat) # multiple outputs by lstm layer.\n",
    "        \n",
    "        # tensor.contiguous() will create a copy of the tensor, and the element in the copy will be stored in the memory in a contiguous(ordered) way.\n",
    "        # contiguous(ordered) => change the order of data in accordance to indices.\n",
    "        # contiguous() function is usually required when we 'changed the shape of a tensor' and further reshaping (view) it. \n",
    "        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n",
    "        \n",
    "        feat = self.head(feat)\n",
    "        feat = feat.view(bs, n_slice_per_c).contiguous()\n",
    "\n",
    "        return feat\n",
    "\n",
    "    \n",
    "    \n",
    "class TimmModelType2(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelType2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.InstanceNorm1d(256), # replaced BatchNorm1d for training with batch_size = 1\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.InstanceNorm1d(256), # replaced BatchNorm1d for training with batch_size = 1\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        x = x.view(bs * n_slice_per_c * 7, in_chans, image_size, image_size)\n",
    "\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * 7, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:06:02.248200Z",
     "iopub.status.busy": "2022-10-30T05:06:02.247561Z",
     "iopub.status.idle": "2022-10-30T05:06:18.464686Z",
     "shell.execute_reply": "2022-10-30T05:06:18.463650Z",
     "shell.execute_reply.started": "2022-10-30T05:06:02.248158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = 'timm3d_effv2_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "models_seg = []\n",
    "\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "len(models_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:11:05.272444Z",
     "iopub.status.busy": "2022-10-30T05:11:05.272040Z",
     "iopub.status.idle": "2022-10-30T05:11:09.237891Z",
     "shell.execute_reply": "2022-10-30T05:11:09.236646Z",
     "shell.execute_reply.started": "2022-10-30T05:11:05.272406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "\n",
    "image_size = 224\n",
    "in_chans = 6\n",
    "models_cls1 = []\n",
    "out_dim = 1\n",
    "drop_rate = 0.\n",
    "drop_rate_last = 0.\n",
    "drop_path_rate = 0.\n",
    "for fold in range(5):\n",
    "    model = TimmModel(backbone, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls1.append(model)\n",
    "\n",
    "len(models_cls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:11:09.240885Z",
     "iopub.status.busy": "2022-10-30T05:11:09.240407Z",
     "iopub.status.idle": "2022-10-30T05:11:11.795864Z",
     "shell.execute_reply": "2022-10-30T05:11:11.794641Z",
     "shell.execute_reply.started": "2022-10-30T05:11:09.240846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_2d_lstmv22headv2_convnn_224_15_6ch_8flip_augv2_drl3_rov1p2_rov3p2_bs4_lr6e5_eta6e6_lw151_50ep'\n",
    "backbone = 'convnext_nano'\n",
    "in_chans = 6\n",
    "models_cls2 = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = TimmModelType2(backbone, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls2.append(model)\n",
    "\n",
    "len(models_cls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:11:12.906693Z",
     "iopub.status.busy": "2022-10-30T05:11:12.906193Z",
     "iopub.status.idle": "2022-10-30T05:11:12.928799Z",
     "shell.execute_reply": "2022-10-30T05:11:12.927486Z",
     "shell.execute_reply.started": "2022-10-30T05:11:12.906652Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_bone(msk, cid, t_paths, cropped_images, index, inst_id):\n",
    "    sema.acquire() #  threading topic    \n",
    "    n_scans = len(t_paths)\n",
    "    bone = []\n",
    "    try:\n",
    "        msk_b = msk[cid] > 0.1\n",
    "            # msk_b.shape => (128, 128, 128)\n",
    "            # msk_b => \n",
    "            # [[[False False False ... False False False]        \n",
    "            #   ...\n",
    "            #   ...\n",
    "            #   [False False False ... False False False]\n",
    "            #   [False False False ... False False False]]]          \n",
    "          \n",
    "        msk_c = msk[cid] > 0.05\n",
    "                \n",
    "            # msk_b.sum(axis = 1).shape => (128, 128)\n",
    "            # msk_b.sum(1).sum(1).shape =>  (128,) \n",
    "            # msk_b.sum(1).sum(1) => \n",
    "            # [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
    "            #   18  75 157 239 307 382 485 592 655 663 688 809 810 601 323 100  36  10\n",
    "            #    0   0   1   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "            #    ...\n",
    "            #    ...\n",
    "            #    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
    "            #    0   0]            \n",
    "            # np.where(msk_b.sum(1).sum(1) > 0) => \n",
    "            # (array([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
    "            #        34, 35, 38, 39]),)              \n",
    "\n",
    "        # finding indices of three coordinate axes of mask_cuboid (128,128,128) where data is present.\n",
    "        # (128,128,128) => (slices, W, H)\n",
    "        y = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "        x = np.where(msk_b.sum(0).sum(1) > 0)[0]        \n",
    "        z = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            y = np.where(msk_c.sum(0).sum(0) > 0)[0]            \n",
    "            x = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(1).sum(1) > 0)[0]            \n",
    "\n",
    "        # msk.shape => (7, 128, 128, 128)\n",
    "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
    "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
    "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
    "\n",
    "        \n",
    "            # z1, z2 => 26 98\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans) # msk_size = 128, defined in config section\n",
    "            # from BODMAS rule => zz1, zz2 => 49 186\n",
    "            # (z1 / msk_size) => proportion of z1-coordinate over index length 128 (msk_size).\n",
    "            # (z1 / msk_size) * n_scans => z1-coordinate over index length 'n_scans'.\n",
    "\n",
    "        ## return 15 (n_slice_per_c) evenly spaced indexes.\n",
    "        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)                 \n",
    "        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n",
    "            # inds_ = np.linspace(26, 98, 128) => [26 31 36 41 46 51 56 61 66 71 76 81 86 91 97]\n",
    "            # inds => [ 49  58  68  78  87  97 107 117 126 136 146 155 165 175 185]\n",
    "            # np.linspace(start, stop, num_of_samples_to_generate, ...) => Return evenly spaced numbers over a specified interval.\n",
    "\n",
    "#         # taking in between range of 128 slices (dcm files).\n",
    "#         inds_ = np.linspace(29 ,101 ,n_slice_per_c).astype(int)\n",
    "#             # inds_ => [ 29  34  39  44  49  54  59  65  70  75  80  85  90  95 101]            \n",
    "\n",
    "\n",
    "        for sid, (ind_, ind) in enumerate(zip(inds_, inds)):\n",
    "#         for sid, (ind_) in enumerate(zip(inds_)):\n",
    "\n",
    "            images = []\n",
    "            for i in range(-n_ch//2+1, n_ch//2+1): # n_ch = 5, defined in config \n",
    "                # for i in (-2, 3):\n",
    "                # 5//2 = 2, -5//2 = -3 (1 extra in -ve)\n",
    "                try:\n",
    "                    dicom = pydicom.read_file(t_paths[ind+i])\n",
    "#                     dicom = pydicom.read_file(t_paths[index[0][ind_[0]+i]])                                        \n",
    "                        # ind_ = 26 read 24, 25, 26, 27, 28 dicom files \n",
    "                        # t_paths[index[0][ind_+i]] => picking files which are also in 'images' during prediction of mask.\n",
    "\n",
    "                    images.append(dicom.pixel_array)\n",
    "                except:\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "\n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "                # data.shape => (512, 512, 5)\n",
    "\n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)  \n",
    "                # prior to any type of transformation(resize, augmentation etc.) convert data to uint8.\n",
    "\n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "\n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "                # data.shape => (96, 172, 5)\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "                # data.shape => (224, 224, 5)                     \n",
    "\n",
    "            if (ind_+n_ch//2) >= msk_size:\n",
    "                msk_this = msk[cid, ind_, :, :] \n",
    "#                 msk_this = msk[cid, ind_[0], :, :] \n",
    "            else:\n",
    "                msk_this = msk[cid, ind_+n_ch//2, :, :] # ind_ = 26            \n",
    "#                 msk_this = msk[cid, ind_[0]+n_ch//2, :, :] # ind_ = 26\n",
    "                # msk[0,26,128,128].shape => (128, 128)         \n",
    "\n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "                # msk_this[16:40, 23:66].shape => (24, 43)                                                \n",
    "            msk_this = (msk_this * 255).astype(np.uint8)\n",
    "                # prior to any type of transformation(resize, augmentation etc.) convert data to uint8.\n",
    "                # np.unique(msk_this).astype(np.uint8)) =>             \n",
    "                # [  0   1   2   4   6   8  11  14  21  23  48  49  50  61  69  72  73  81\n",
    "                #  107 109 113 116 121 128 143 144 147 161 181 192 204 207 209 214 232 236\n",
    "                #  240 241 245 247 250 252 253 254 255]                                \n",
    "\n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)            \n",
    "                # msk_this.shape => (224, 224)\n",
    "\n",
    "            # appending mask at as 6th channel of data. \n",
    "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)            \n",
    "                 # data.shape => (224, 224, 6)\n",
    "            \n",
    "            bone.append(torch.tensor(data))            \n",
    "\n",
    "    except:\n",
    "        for sid in range(n_slice_per_c):\n",
    "            bone.append(torch.zeros((image_size_cls, image_size_cls, n_ch+1)).to(torch.uint8))            \n",
    "                \n",
    "    cropped_images[cid] = torch.stack(bone, 0)    \n",
    "        # cropped_images[cid].shape => torch.Size([15, 224, 224, 6])    \n",
    "        \n",
    "    ### using local cache\n",
    "#     image_file = os.path.join(data_dir, f'numpy_1/{inst_id}_{cid+1}.npy')\n",
    "#     np.save(image_file, cropped_images[cid])\n",
    "    time.sleep(2) #  threading topic\n",
    "    sema.release() #  threading topic    \n",
    "\n",
    "def load_cropped_images(msk, image_folder, index, inst_id, n_ch=n_ch):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    \n",
    "    for cid in range(7): # 7\n",
    "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images, index, inst_id))\n",
    "        threads[cid].start()\n",
    "    for cid in range(7):\n",
    "        threads[cid].join()\n",
    "\n",
    "        # torch.cat(tensors, dim=0,..) => Concatenates the given sequence of seq tensors in the given dimension. \n",
    "        # torch.cat(cropped_images, 0).shape => torch.Size([105, 224, 224, 6])    \n",
    "    return torch.cat(cropped_images, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sema = threading.Semaphore(value=12) # value => setting number of threads\n",
    "dataset_seg = SegTestDataset(df) # df[3:4] df[1:2]\n",
    "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-30T05:11:35.770395Z",
     "iopub.status.busy": "2022-10-30T05:11:35.770051Z",
     "iopub.status.idle": "2022-10-30T05:11:43.226734Z",
     "shell.execute_reply": "2022-10-30T05:11:43.225225Z",
     "shell.execute_reply.started": "2022-10-30T05:11:35.770366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:27<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs1 = []\n",
    "outputs2 = []\n",
    "\n",
    "bar = tqdm(loader_seg)\n",
    "with torch.no_grad():\n",
    "    for batch_id, (images, indices) in enumerate(bar):\n",
    "        indices = indices.numpy()\n",
    "        images = images.cuda()\n",
    "\n",
    "        # SEG\n",
    "        pred_masks = []\n",
    "        for model in models_seg:\n",
    "            pmask = model(images)\n",
    "            pmask = pmask.sigmoid()\n",
    "            pred_masks.append(pmask)\n",
    "        pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n",
    "        \n",
    "        # Build cls input\n",
    "        cls_inp = []\n",
    "        threads = [None] * 7\n",
    "        cropped_images = [None] * 7\n",
    "\n",
    "        for i in range(pred_masks.shape[0]):\n",
    "            row = df.iloc[batch_id*batch_size_seg+i]\n",
    "            cropped_images = load_cropped_images(pred_masks[i], row.image_folder, indices, row.StudyInstanceUID)\n",
    "#             cropped_images = cropped_images[:105,:,:,:] # torch.Size([119, 224, 224, 6]) reduces to torch.Size([105, 224, 224, 6])\n",
    "            cls_inp.append((cropped_images.permute(0, 3, 1, 2) / 255.).float())\n",
    "        cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 224, 224)\n",
    "                \n",
    "        pred_cls1, pred_cls2 = [], []\n",
    "        # CLS 2\n",
    "        for _, model in enumerate(models_cls2):\n",
    "            logits, logits2 = model(cls_inp)\n",
    "            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "            pred_cls2.append(logits2.sigmoid())\n",
    "            \n",
    "        # CLS 1\n",
    "        cls_inp = cls_inp.view(7, 15, 6, image_size_cls, image_size_cls).contiguous()\n",
    "        for _, model in enumerate(models_cls1):\n",
    "            logits = model(cls_inp)          \n",
    "            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "        \n",
    "        pred_cls1 = torch.stack(pred_cls1, 0).mean(0)\n",
    "        pred_cls2 = torch.stack(pred_cls2, 0).mean(0)\n",
    "        outputs1.append(pred_cls1.cpu())\n",
    "            # len(outputs1) = no. of test patients = 3\n",
    "            # outputs1[0].shape => torch.Size([1, 7, 15])\n",
    "        outputs2.append(pred_cls2.cpu())\n",
    "            # len(outputs2) = no. of test patients = 3\n",
    "            # outputs2 => [tensor([[0.4593]]), tensor([[0.0320]]), ..., tensor([[0.2525]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.096666Z",
     "iopub.status.busy": "2022-10-21T11:38:43.095123Z",
     "iopub.status.idle": "2022-10-21T11:38:43.119061Z",
     "shell.execute_reply": "2022-10-21T11:38:43.118069Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.096623Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs1 = torch.cat(outputs1)\n",
    "    # concatenates list elements to tensor.\n",
    "    # outputs1.shape => torch.Size([3, 7, 15])\n",
    "    # outputs1[0].shape => torch.Size([7, 15])\n",
    "    # outputs1[0] => \n",
    "    # tensor([[0.0365, 0.0338, 0.0387, 0.0372, 0.0386, 0.0373, 0.0376, 0.0375, 0.0406,\n",
    "    #          0.0466, 0.0511, 0.0583, 0.0711, 0.0731, 0.0946],\n",
    "    #         ...\n",
    "    #         ...\n",
    "    #         [0.2809, 0.2936, 0.2806, 0.2562, 0.2504, 0.2489, 0.2307, 0.2511, 0.2575,\n",
    "    #          0.2629, 0.2683, 0.2799, 0.2696, 0.2700, 0.2663]])    \n",
    "outputs2 = torch.cat(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.121396Z",
     "iopub.status.busy": "2022-10-21T11:38:43.120761Z",
     "iopub.status.idle": "2022-10-21T11:38:43.135306Z",
     "shell.execute_reply": "2022-10-21T11:38:43.134234Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.121355Z"
    }
   },
   "outputs": [],
   "source": [
    "PRED1 = (outputs1.mean(-1)).clamp(0.0001, 0.9999)\n",
    "    # outputs1.mean(-1).shape => torch.Size([3, 7])\n",
    "    # outputs1[0].mean(-1).shape => torch.Size([7])\n",
    "    # .clamp(0.0001, 0.9999) => clamp all inputs in the range\n",
    "    # PRED1.shape => torch.Size([3, 7])\n",
    "PRED2 = (outputs2.view(-1)).clamp(0.0001, 0.9999)\n",
    "    # PRED2.shape => torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.137361Z",
     "iopub.status.busy": "2022-10-21T11:38:43.136776Z",
     "iopub.status.idle": "2022-10-21T11:38:43.14516Z",
     "shell.execute_reply": "2022-10-21T11:38:43.144134Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.137324Z"
    }
   },
   "outputs": [],
   "source": [
    "row_ids = []\n",
    "for _, row in df.iterrows():\n",
    "    for i in range(7):\n",
    "        row_ids.append(row.StudyInstanceUID + f'_C{i+1}')\n",
    "    row_ids.append(row.StudyInstanceUID + '_patient_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.147449Z",
     "iopub.status.busy": "2022-10-21T11:38:43.146675Z",
     "iopub.status.idle": "2022-10-21T11:38:43.154873Z",
     "shell.execute_reply": "2022-10-21T11:38:43.153934Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.147397Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    'row_id': row_ids,\n",
    "    'fractured': torch.cat([PRED1, PRED2.unsqueeze(1)], 1).view(-1),\n",
    "    # torch.cat([PRED1, PRED2.unsqueeze(1)], 1).shape => torch.Size([3, 8])\n",
    "    # PRED2.unsqueeze(1).shape => torch.Size([3, 1])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.15722Z",
     "iopub.status.busy": "2022-10-21T11:38:43.156122Z",
     "iopub.status.idle": "2022-10-21T11:38:43.167857Z",
     "shell.execute_reply": "2022-10-21T11:38:43.166934Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.157186Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T11:38:43.17Z",
     "iopub.status.busy": "2022-10-21T11:38:43.169507Z",
     "iopub.status.idle": "2022-10-21T11:38:43.182118Z",
     "shell.execute_reply": "2022-10-21T11:38:43.181084Z",
     "shell.execute_reply.started": "2022-10-21T11:38:43.169962Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>fractured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C1</td>\n",
       "      <td>0.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C2</td>\n",
       "      <td>0.048528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C3</td>\n",
       "      <td>0.009748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C4</td>\n",
       "      <td>0.016578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C5</td>\n",
       "      <td>0.032510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C6</td>\n",
       "      <td>0.100792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C7</td>\n",
       "      <td>0.493738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_patient_overall</td>\n",
       "      <td>0.565615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C1</td>\n",
       "      <td>0.012958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C2</td>\n",
       "      <td>0.013387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C3</td>\n",
       "      <td>0.030004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C4</td>\n",
       "      <td>0.029569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C5</td>\n",
       "      <td>0.055547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C6</td>\n",
       "      <td>0.047669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C7</td>\n",
       "      <td>0.078027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_patient_overall</td>\n",
       "      <td>0.186130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C1</td>\n",
       "      <td>0.015534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C2</td>\n",
       "      <td>0.052275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C3</td>\n",
       "      <td>0.062867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C4</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C5</td>\n",
       "      <td>0.018639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C6</td>\n",
       "      <td>0.013161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C7</td>\n",
       "      <td>0.036474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_patient_overall</td>\n",
       "      <td>0.458445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       row_id  fractured\n",
       "0                1.2.826.0.1.3680043.22327_C1   0.014081\n",
       "1                1.2.826.0.1.3680043.22327_C2   0.048528\n",
       "2                1.2.826.0.1.3680043.22327_C3   0.009748\n",
       "3                1.2.826.0.1.3680043.22327_C4   0.016578\n",
       "4                1.2.826.0.1.3680043.22327_C5   0.032510\n",
       "5                1.2.826.0.1.3680043.22327_C6   0.100792\n",
       "6                1.2.826.0.1.3680043.22327_C7   0.493738\n",
       "7   1.2.826.0.1.3680043.22327_patient_overall   0.565615\n",
       "8                1.2.826.0.1.3680043.25399_C1   0.012958\n",
       "9                1.2.826.0.1.3680043.25399_C2   0.013387\n",
       "10               1.2.826.0.1.3680043.25399_C3   0.030004\n",
       "11               1.2.826.0.1.3680043.25399_C4   0.029569\n",
       "12               1.2.826.0.1.3680043.25399_C5   0.055547\n",
       "13               1.2.826.0.1.3680043.25399_C6   0.047669\n",
       "14               1.2.826.0.1.3680043.25399_C7   0.078027\n",
       "15  1.2.826.0.1.3680043.25399_patient_overall   0.186130\n",
       "16                1.2.826.0.1.3680043.5876_C1   0.015534\n",
       "17                1.2.826.0.1.3680043.5876_C2   0.052275\n",
       "18                1.2.826.0.1.3680043.5876_C3   0.062867\n",
       "19                1.2.826.0.1.3680043.5876_C4   0.055800\n",
       "20                1.2.826.0.1.3680043.5876_C5   0.018639\n",
       "21                1.2.826.0.1.3680043.5876_C6   0.013161\n",
       "22                1.2.826.0.1.3680043.5876_C7   0.036474\n",
       "23   1.2.826.0.1.3680043.5876_patient_overall   0.458445"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

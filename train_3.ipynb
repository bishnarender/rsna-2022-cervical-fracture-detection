{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding JPEG images and decoding/encoding RLE datasets\n",
    "# !pip3 install pylibjpeg==1.4.0\n",
    "# https://github.com/pydicom/pylibjpeg\n",
    "\n",
    "# !pip3 install python-gdcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suitable for kaggle notebook\n",
    "# sys.path = ['../ca_2',] + sys.path\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, ast, cv2, time, pickle, random\n",
    "# import pylibjpeg\n",
    "# import gdcm\n",
    "# import pydicom\n",
    "# pydicom is a pure Python package for working with DICOM files. \n",
    "# -It lets you read, modify and write DICOM data in an easy \"pythonic\" way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# import nibabel as nib\n",
    "# read / write access to some common neuroimaging file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import albumentations # python library for pixel-level augmentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "# from timm0412 import timm as timm # timm0412 means timm v0.4.12\n",
    "\n",
    "# import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip3 install torchview\n",
    "# from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_column', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_seq_items', None)\n",
    "# pd.set_option('display.max_colwidth', None) # 500\n",
    "# pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "# benchmark mode is good whenever your input sizes for your network do not vary. \n",
    "# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T09:08:50.228479Z",
     "iopub.status.busy": "2022-10-29T09:08:50.228099Z",
     "iopub.status.idle": "2022-10-29T09:08:50.237679Z",
     "shell.execute_reply": "2022-10-29T09:08:50.235921Z",
     "shell.execute_reply.started": "2022-10-29T09:08:50.228447Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_type = '0920_2d_lstmv22headv2_convnn_224_15_6ch_8flip_augv2_drl3_rov1p2_rov3p2_bs4_lr6e5_eta6e6_lw151_50ep'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "\n",
    "n_folds = 5\n",
    "backbone = 'convnext_nano'\n",
    "\n",
    "image_size = 224\n",
    "n_slice_per_c = 15\n",
    "in_chans = 6\n",
    "\n",
    "init_lr = 18739e-10 # 18739e-9 , 18739e-10 (run at peak)\n",
    "eta_min = 18700e-10 # 18700e-9\n",
    "lw = [15, 1]\n",
    "batch_size = 1\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "drop_rate_last = 0.\n",
    "p_mixup = 0.5 \n",
    "p_rand_order = 0.5 \n",
    "p_rand_order_v1 = 0.\n",
    "weight_decay=0 # default: depend upon optimizer, regularizer like dropout to prevent overfitting.\n",
    "n_accumulate=1 # \n",
    "\n",
    "data_dir = './'\n",
    "use_amp = True\n",
    "num_workers = 11 \n",
    "out_dim = 1\n",
    "\n",
    "n_epochs = 81 # 80\n",
    "\n",
    "log_dir = './logs'\n",
    "model_dir = './models'\n",
    "model_dir_seg = './kaggle'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T09:08:51.019330Z",
     "iopub.status.busy": "2022-10-29T09:08:51.018970Z",
     "iopub.status.idle": "2022-10-29T09:08:51.028636Z",
     "shell.execute_reply": "2022-10-29T09:08:51.027512Z",
     "shell.execute_reply.started": "2022-10-29T09:08:51.019300Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Albumentations is a computer vision tool that boosts the performance of deep convolutional neural networks.\n",
    "# # Albumentations is a Python library for image augmentation.\n",
    "# # preferred border_mode=\"reflection\" for all techniques.\n",
    "transforms_train = albumentations.Compose([\n",
    "#     albumentations.Resize(image_size, image_size),    \n",
    "    albumentations.OneOf([    \n",
    "        albumentations.HorizontalFlip(p=1.),\n",
    "        albumentations.VerticalFlip(p=1.),\n",
    "        albumentations.Transpose(p=1.),        \n",
    "    ], p=0.8),\n",
    "   \n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomGamma(gamma_limit=(100, 150), p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.3, p=0.5),\n",
    "        albumentations.Equalize(p=0.5),\n",
    "        albumentations.Sharpen(alpha=(0.5, 1.), lightness=(0.5, 1.0), p=0.5),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, border_mode=4, p=0.5), \n",
    "        \n",
    "#         albumentations.GaussNoise(var_limit=(9000.0, 9999.0), p=1.),\n",
    "        albumentations.Defocus(radius=(8, 8), alias_blur=(0.4, 0.4), p=0.4),\n",
    "        albumentations.MotionBlur(blur_limit=27, p=0.4),\n",
    "        albumentations.MedianBlur(blur_limit=27, p=0.4),\n",
    "        albumentations.GaussianBlur(blur_limit=(25,27), p=0.4),\n",
    "        albumentations.GlassBlur(sigma=0.7, max_delta=4, iterations=2, mode='fast', p=0.4),\n",
    "        albumentations.ElasticTransform(alpha=80, sigma=6, alpha_affine=6, p=0.4),\n",
    "        albumentations.ElasticTransform(alpha=20, sigma=80, alpha_affine=80, p=0.4),\n",
    "        albumentations.GridDistortion(num_steps=5, distort_limit=(-0.2,0.2), p=0.4),\n",
    "        albumentations.OpticalDistortion(distort_limit=(-0.3,0.3), shift_limit=(-0.5,0.5), p=0.4),\n",
    "        albumentations.CoarseDropout(max_height=int(image_size * 0.5), max_width=int(image_size * 0.5), max_holes=1, fill_value=1., p=0.4),\n",
    "    ], p=1.),\n",
    "])\n",
    "\n",
    "transforms_valid = albumentations.Compose([\n",
    "##     albumentations.Resize(image_size, image_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T09:08:51.859289Z",
     "iopub.status.busy": "2022-10-29T09:08:51.858480Z",
     "iopub.status.idle": "2022-10-29T09:08:51.886459Z",
     "shell.execute_reply": "2022-10-29T09:08:51.885393Z",
     "shell.execute_reply.started": "2022-10-29T09:08:51.859244Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_seg.csv')\n",
    "df = df.sample(16).reset_index(drop=True) if DEBUG else df\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>image_folder</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>d</th>\n",
       "      <th>t</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.27262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>406</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n",
       "1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0   \n",
       "\n",
       "  mask_file                                       image_folder    w    h    d  \\\n",
       "1       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512  406   \n",
       "\n",
       "     t  fold  \n",
       "1  0.5     0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>image_folder</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>d</th>\n",
       "      <th>t</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2.826.0.1.3680043.4744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>281</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2.826.0.1.3680043.15773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>185</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.2.826.0.1.3680043.24946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>571</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.2.826.0.1.3680043.9290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>223</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.2.826.0.1.3680043.5482</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/rsna-2022-cervical-spine-fracture-detect...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>234</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n",
       "10   1.2.826.0.1.3680043.4744                1   0   0   0   1   1   0   0   \n",
       "11  1.2.826.0.1.3680043.15773                1   1   1   0   0   0   0   1   \n",
       "12  1.2.826.0.1.3680043.24946                0   0   0   0   0   0   0   0   \n",
       "13   1.2.826.0.1.3680043.9290                0   0   0   0   0   0   0   0   \n",
       "14   1.2.826.0.1.3680043.5482                1   1   0   0   0   0   0   0   \n",
       "\n",
       "   mask_file                                       image_folder    w    h  \\\n",
       "10       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512   \n",
       "11       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512   \n",
       "12       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512   \n",
       "13       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512   \n",
       "14       NaN  /data/rsna-2022-cervical-spine-fracture-detect...  512  512   \n",
       "\n",
       "      d      t  fold  \n",
       "10  281  0.625     0  \n",
       "11  185  1.000     0  \n",
       "12  571  0.500     0  \n",
       "13  223  1.000     0  \n",
       "14  234  0.625     0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T09:08:52.664349Z",
     "iopub.status.busy": "2022-10-29T09:08:52.663996Z",
     "iopub.status.idle": "2022-10-29T09:08:52.675444Z",
     "shell.execute_reply": "2022-10-29T09:08:52.674271Z",
     "shell.execute_reply.started": "2022-10-29T09:08:52.664319Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLSDataset(Dataset):\n",
    "    def __init__(self, df, mode, transform):\n",
    "\n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        images_lst = []\n",
    "        \n",
    "        tmp = list(range(7))\n",
    "        ### random order v3\n",
    "        if self.mode == 'train' and random.random() < p_rand_order:\n",
    "            _ = random.shuffle(tmp)\n",
    "        ###\n",
    "        for cid in (tmp):                \n",
    "            filepath = os.path.join(data_dir, f'numpy_2/{row.StudyInstanceUID}_{cid+1}.npy')\n",
    "            images = np.load(filepath)            \n",
    "                # type(image), image.shape => <class 'numpy.ndarray'> (15, 224, 224, 6)\n",
    "\n",
    "            images = np.stack([self.transform(image=images[i])['image'] for i in range(n_slice_per_c)], 0)\n",
    "\n",
    "            images = images.transpose(0,3,1,2)\n",
    "                # type(image), image.shape => <class 'numpy.ndarray'> (15, 6, 224, 224)        \n",
    "\n",
    "            images = images / 255. # trim the 'data values' between 0. and 1. \n",
    "                 \n",
    "            images_lst.append(images)                \n",
    "            \n",
    "        images_lst = np.stack(images_lst, 0)\n",
    "\n",
    "        images_lst = images_lst.reshape((105,6,224,224)).astype(np.float32)\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            labels = []\n",
    "            # tmp => [0, 1, 2, 3, 4, 5, 6]\n",
    "            for i in row[[f'C{x+1}' for x in tmp]].tolist():\n",
    "                labels += [i] * n_slice_per_c\n",
    "                \n",
    "            # labels => [1, 1, 1, 1, 1, 1, 1, 1, .........., 0, 0, 0, 0, 0, 0, 0, 0] => 105 items\n",
    "\n",
    "            images = torch.tensor(images_lst)#.float()\n",
    "            labels = torch.tensor(labels).float()\n",
    "            \n",
    "            if self.mode == 'train' and random.random() < p_rand_order_v1:\n",
    "                indices = torch.randperm(images.size(0))\n",
    "                images = images_lst[indices]\n",
    "                labels = labels[indices]\n",
    "\n",
    "            return images, labels\n",
    "        else:\n",
    "            return torch.tensor(images)#.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_show = df[163:164]\n",
    "# dataset_show = CLSDataset(df_show, 'train', transform=transforms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa, bb = dataset_show[0]\n",
    "# # aa.shape\n",
    "# # print(torch.max(aa), torch.min(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all Cid (for one patient) one by one in order to verify that images are showing correctly or not -\n",
    "# # - otherwise adjust numeric value in 'msk[cid] > 0.1' in above function.\n",
    "# # plotter Cid = 2\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "# for i in range(10,20):\n",
    "#     fx, arr = plt.subplots(1,6)\n",
    "    \n",
    "#     for j in range(6):\n",
    "#         arr[j].imshow(aa[i][j,:,:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T09:09:24.701871Z",
     "iopub.status.busy": "2022-10-29T09:09:24.700750Z",
     "iopub.status.idle": "2022-10-29T09:09:24.715451Z",
     "shell.execute_reply": "2022-10-29T09:09:24.714120Z",
     "shell.execute_reply.started": "2022-10-29T09:09:24.701808Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimmModelType2(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelType2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.InstanceNorm1d(256), # replaced BatchNorm1d for training with batch_size = 1\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),#0.1\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.InstanceNorm1d(256), # replaced BatchNorm1d for training with batch_size = 1\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),#0.1\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        x = x.view(bs * n_slice_per_c * 7, in_chans, image_size, image_size)\n",
    "\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * 7, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = TimmModelType2(backbone)\n",
    "# n_sequence = 7 * n_slice_per_c\n",
    "# m(torch.rand(2, n_sequence, in_chans, image_size, image_size)).shape\n",
    "# #     m(torch.rand(2, n_sequence, in_chans, image_size, image_size)).shape => torch.Size([2, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_graph(m, input_data = torch.rand(1, 3, 128,128,128), expand_nested=True, save_graph=True).visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "\n",
    "def criterion(logits, targets, activated=False):\n",
    "    # logits.view(-1) => have negative values.\n",
    "\n",
    "    if activated:\n",
    "        losses = nn.BCELoss(reduction='none')(logits.view(-1), targets.view(-1))\n",
    "            # .view(-1) => return a single dimension tensor.\n",
    "            # .view() => Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "    else:\n",
    "        losses = bce(logits.view(-1), targets.view(-1))\n",
    "    losses[targets.view(-1) > 0] *= 2.\n",
    "         # losses[targets.view(-1) > 0] => selecting those indices values from losses, where the targets are '> 0'.\n",
    "         # losses[targets.view(-1) > 0] *= 2. =>   losses[targets.view(-1) > 0] = losses[targets.view(-1) > 0] * 2.\n",
    "    norm = torch.ones(logits.view(-1).shape[0]).to(device)\n",
    "    norm[targets.view(-1) > 0] *= 2\n",
    "\n",
    "    return losses.sum() / norm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Valid func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T08:11:53.858498Z",
     "iopub.status.busy": "2022-10-29T08:11:53.858143Z",
     "iopub.status.idle": "2022-10-29T08:11:53.87108Z",
     "shell.execute_reply": "2022-10-29T08:11:53.869966Z",
     "shell.execute_reply.started": "2022-10-29T08:11:53.858468Z"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle the original input then mix this shuffled with original input.\n",
    "def mixup(input_, truth, clip=[0.0, 1.0]):\n",
    "    # torch.randperm(n, *, ...) => Returns a random permutation of integers from 0 to n - 1.    \n",
    "    indices = torch.randperm(input_.size(0))\n",
    "    \n",
    "    # shuffling batch in batch of images.     \n",
    "    shuffled_input = input_[indices]    \n",
    "    # shuffling batch in batch of masks.     \n",
    "    shuffled_labels = truth[indices]\n",
    "\n",
    "    # np.random.uniform(low=0.0, high=1.0, size=None) => draw sample(s) from a uniform distribution over the over the half-open interval [low, high).\n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    \n",
    "    # mixing 'image batch' with 'shuffled image batch' i.e., type of image transformation.   \n",
    "    input_ = input_ * lam + shuffled_input * (1 - lam)\n",
    "\n",
    "    return input_, shuffled_labels, lam\n",
    "\n",
    "\n",
    "def train_func(model, loader_train, optimizer, scaler=None):    \n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_loss1 = []\n",
    "    train_loss2 = []\n",
    "    bar = tqdm(loader_train)\n",
    "    \n",
    "    i=0\n",
    "    for images, targets in bar:\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        do_mixup = False\n",
    "        if random.random() < p_mixup:\n",
    "            do_mixup = True\n",
    "            images, targets_mix, lam = mixup(images, targets)\n",
    "\n",
    "        with amp.autocast():            \n",
    "            logits, logits2 = model(images)\n",
    "                # logits => tensor([[-1.1621],[-0.7876],[-0.7744],[-0.6548],[-0.8032], ...., [-0.5107], [-0.0616]], device='cuda:0',...)  \n",
    "                # logits2 => tensor([[0.3247]], device='cuda:0',..)\n",
    "                \n",
    "                # targets.max() => tensor(1., device='cuda:0')\n",
    "                # targets.max(1) => torch.return_types.max(values=tensor([1.], device='cuda:0'),indices=tensor([0], device='cuda:0'))\n",
    "                # targets.max(1).values => tensor([1.], device='cuda:0')  \n",
    "                # targets.max(dim=1) => Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim=1. \n",
    "                \n",
    "            if do_mixup: targets = targets * lam + targets_mix * (1 - lam)\n",
    "            \n",
    "            loss1 = criterion(logits, targets)\n",
    "            loss2 = criterion(logits2, targets.max(1).values)\n",
    "                # loss1 => tensor(0.7388, device='cuda:0', grad_fn=<DivBackward0>) \n",
    "                # loss2 => tensor(0.7343, device='cuda:0', grad_fn=<DivBackward0>)                \n",
    "\n",
    "            #loss = (loss1 * lw[0] + loss2 * lw[1]) / sum(lw)       \n",
    "            loss = loss1 + loss2 / 2.\n",
    "            \n",
    "#             if do_mixup:\n",
    "#                 loss11 = criterion(logits, targets_mix)\n",
    "#                 loss21 = criterion(logits2, targets_mix.max(1).values)\n",
    "#                 #loss = loss * lam  + (loss11 * lw[0] + loss21 * lw[1]) / sum(lw) * (1 - lam) \n",
    "#                 loss = loss * lam  + (loss11  + loss21 ) / 2. * (1 - lam) \n",
    "                \n",
    "        train_loss1.append(loss1.item())\n",
    "        train_loss2.append(loss2.item())\n",
    "        train_loss.append(loss.item())\n",
    "        scaler.scale(loss).backward() # retain_graph=True \n",
    "        \n",
    "        \n",
    "        if (i + 1) % n_accumulate == 0:            \n",
    "            scaler.step(optimizer)\n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.        \n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped.\n",
    "            # optimizer's assigned params; parameters which are to be optimized by optimizer.\n",
    "        \n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            \n",
    "            # to reset the gradients of model parameters.             \n",
    "            optimizer.zero_grad()   \n",
    "            i=-1\n",
    "        i+=1        \n",
    "\n",
    "        bar.set_description(f'smth:{np.mean(train_loss1[-30:]):.4f} {np.mean(train_loss2[-30:]):.4f}')\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def valid_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    valid_loss1 = []\n",
    "    valid_loss2 = []\n",
    "    outputs = []\n",
    "    bar = tqdm(loader_valid)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in bar:\n",
    "            images = images.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            logits, logits2 = model(images)\n",
    "            loss1 = criterion(logits, targets)\n",
    "            loss2 = criterion(logits2, targets.max(1).values)\n",
    "            \n",
    "            loss = (loss1 + loss2) / 2.\n",
    "            #loss = (loss1* lw[0] + loss2* lw[1]) / sum(lw)\n",
    "#             valid_loss1.append(loss1.item())\n",
    "#             valid_loss2.append(loss2.item())\n",
    "            valid_loss.append(loss.item())\n",
    "#             bar.set_description(f'smth:{np.mean(valid_loss1[-30:]):.4f} {np.mean(valid_loss2[-30:]):.4f}')\n",
    "\n",
    "    return np.mean(valid_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = TimmModelType2(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T08:11:55.434135Z",
     "iopub.status.busy": "2022-10-29T08:11:55.433782Z",
     "iopub.status.idle": "2022-10-29T08:11:55.637241Z",
     "shell.execute_reply": "2022-10-29T08:11:55.636298Z",
     "shell.execute_reply.started": "2022-10-29T08:11:55.434105Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.AdamW(m.parameters(), lr=14e-7)\n",
    "# scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 30, eta_min = 13e-7)\n",
    "# # scheduler_cosine = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.33, total_iters=22)\n",
    "\n",
    "# lrs = []\n",
    "# for epoch in range(1, 30+1):\n",
    "#     scheduler_cosine.step(epoch-1)\n",
    "#     lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "# plt.plot(range(len(lrs)), lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T08:12:38.503712Z",
     "iopub.status.busy": "2022-10-29T08:12:38.503102Z",
     "iopub.status.idle": "2022-10-29T08:12:38.522409Z",
     "shell.execute_reply": "2022-10-29T08:12:38.521354Z",
     "shell.execute_reply.started": "2022-10-29T08:12:38.503668Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "\n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "\n",
    "    train_ = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    valid_ = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "#     # for debugging\n",
    "#     train_ = df[df['fold'] != fold][:100].reset_index(drop=True)\n",
    "#     valid_ = df[df['fold'] != fold][100:120].reset_index(drop=True)\n",
    "\n",
    "    dataset_train = CLSDataset(train_, 'train', transform=transforms_train)\n",
    "    dataset_valid = CLSDataset(valid_, 'valid', transform=transforms_valid)\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = TimmModelType2(backbone, pretrained=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if not first run, load previous model\n",
    "    fold_l = 4#1\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold_l}_last.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=False)    \n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    from_epoch = 0\n",
    "    metric_best = 0.35\n",
    "    loss_min = np.inf\n",
    "\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs, eta_min=eta_min)\n",
    "\n",
    "#     print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        scheduler_cosine.step(epoch-1)\n",
    "        if epoch < from_epoch + 1:\n",
    "            print(logs[epoch-1])\n",
    "            continue\n",
    "\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "        train_loss = train_func(model, loader_train, optimizer, scaler)\n",
    "        valid_loss = valid_func(model, loader_valid)\n",
    "        metric = valid_loss\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n",
    "        print(content)\n",
    "        with open(log_file, 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        if metric < metric_best:#abs(train_loss-valid_loss) <= 0.009:\n",
    "            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n",
    "            if not DEBUG:\n",
    "                torch.save(model.state_dict(), model_file)\n",
    "                metric_best = metric\n",
    "\n",
    "        # Save Last\n",
    "        if not DEBUG and abs(train_loss-valid_loss) <= 0.015:\n",
    "            torch.save(model.state_dict(), model_file.replace('_best', '_last'))\n",
    "#            torch.save(\n",
    "#                {\n",
    "#                    'epoch': epoch,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "#                     'score_best': metric_best,\n",
    "#                 },\n",
    "#                 model_file.replace('_best', '_last')\n",
    "#             )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    _ = gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T08:12:42.821672Z",
     "iopub.status.busy": "2022-10-29T08:12:42.821292Z"
    }
   },
   "outputs": [],
   "source": [
    "#run(1)\n",
    "#run(2)\n",
    "#run(0)\n",
    "#run(3)\n",
    "run(4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  train loss: 0.05, valid loss: 0.01, fold1 last both kaggle\n",
    "\n",
    "#  train loss: 0.05656, valid loss: 0.05088, fold4 last both kaggle\n",
    "\n",
    "#  train loss: 0.06990, valid loss: 0.02214, fold0 last both kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
